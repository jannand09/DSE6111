blog2.pred <- rep("below", length(test.bos))
blog2.pred[blog.probs < 0.5] <- "above"
table(blog2.pred, mcrim.test)
# LDA using rad, lstat, indus, black, and medv
lda.boston <- lda(mcrim ~ rad + lstat + indus + black + medv,
data = boston, subset = train.ind)
lda_bos.pred <- predict(lda.boston, test.bos)
lda_bos.class <- lda_bos.pred$class
table(lda_bos.class, mcrim.test)
(46 + 55) / (46 + 55 + 22 + 4)
# LDA using just lstat and medv
lda.boston2 <- lda(mcrim ~ lstat + medv,
data = boston, subset = train.ind)
lda_bos2.pred <- predict(lda.boston2, test.bos)
lda_bos2.class <- lda_bos2.pred$class
table(lda_bos2.class, mcrim.test)
(39 + 46) / (39 + 46 + 29 + 13)
# Naive Bayes using rad, lstat, indus, black, and medv
nb_bos.fit <- naiveBayes(mcrim ~ rad + lstat + indus + black + medv,
data = boston, subset = train.ind)
nb_bos.class <- predict(nb_bos.fit, test.bos)
table(nb_bos.class, mcrim.test)
(42 + 56) / (42 + 56 + 26 + 3)
# Naive Bayes using just lstat and medv
nb_bos2.fit <- naiveBayes(mcrim ~ lstat + medv,
data = boston, subset = train.ind)
nb_bos2.class <- predict(nb_bos2.fit, test.bos)
table(nb_bos2.class, mcrim.test)
(41 + 48) / (41 + 48 + 27 + 11)
# KNN using rad, lstat, indus, black, and medv
bos.train <- cbind(boston$rad, boston$lstat, boston$indus, boston$black, boston$medv)[train.ind, ]
bos.test <- cbind(boston$rad, boston$lstat, boston$indus, boston$black, boston$medv)[-train.ind, ]
bos.mcrim <- boston$mcrim[train.ind]
set.seed(3)
knn_bos.pred <- knn(bos.train, bos.test, bos.mcrim, k = 1)
table(knn_bos.pred, mcrim.test)
(58 + 47) / (58 + 47 + 10 + 12)
# KNN using rad, lstat, black, and medv k = 3
set.seed(3)
knn_bos2.pred <- knn(bos.train, bos.test, bos.mcrim, k = 3)
table(knn_bos2.pred, mcrim.test)
(56 + 49) / (56 + 49 + 12 + 10)
# KNN using lstat and medv
bos2.train <- cbind(boston$lstat, boston$medv)[train.ind, ]
bos2.test <- cbind(boston$lstat, boston$medv)[-train.ind, ]
set.seed(3)
knn_bos3.pred <- knn(bos2.train, bos2.test, bos.mcrim, k = 3)
table(knn_bos3.pred, mcrim.test)
(45 + 40) / (45 + 40 + 23 + 19)
knitr::opts_chunk$set(echo = TRUE)
library(ISLR2)
library(MASS)
library(boot)
boston.data <- Boston
u_hat <- mean(boston.data$medv)
se_hat <- sd(boston.data$medv) / sqrt(nrow(boston.data$medv))
se_hat <- sd(boston.data$medv) / sqrt(nrow(boston.data))
se_hat
u_medv.fn <- function(data, variable, index) {
sum(data$variable[index]) / length(index)
}
u_medv.fn(boston.data, medv, 1:100)
u_medv.fn <- function(data, variable, index) {
mean(data$variable[index])
}
u_medv.fn(boston.data, medv, 1:100)
u_medv.fn <- function(data, variable, index) {
mean(data$variable[index])
}
x <- u_medv.fn(boston.data, medv, 1:100)
x <- u_medv.fn(boston.data, medv, 1:100)
boston$medv[1:100]
library(ISLR2)
library(MASS)
library(boot)
boston.data <- Boston
boston$medv[1:100]
boston.data$medv[1:100]
y <- boston.data$medv[1:100]
mean(y)
u_medv.fn <- function(data, variable, index) {
+  mean(data$variable[index])
}
x <- u_medv.fn(boston.data, medv, 1:100)
u_medv.fn <- function(data, variable, index) {
z <- data$variable[index]
class(z)
}
x <- u_medv.fn(boston.data, medv, 1:100)
u_medv.fn <- function(data, index) {
z <- data$medv[index]
class(z)
}
x <- u_medv.fn(boston.data, 1:100)
u_medv.fn <- function(data, index) {
z <- data$medv[index]
sum(z) / length(z)
}
x <- u_medv.fn(boston.data, 1:100)
u_medv.fn <- function(data, index) {
z <- data$medv[index]
sum(z) / length(z)
}
boot(boston.data, u_medv.fn, R = 1000)
u_hat <- mean(boston.data$medv)
u_hat
se_hat <- sd(boston.data$medv) / sqrt(nrow(boston.data))
se_hat
?boot.ci
u_medv.fn <- function(data, index) {
z <- data$medv[index]
sum(z) / length(z)
}
# Bootstrap calcuation for part b
boot.b <- boot(boston.data, u_medv.fn, R = 1000)
boot.b
u_medv.fn <- function(data, index) {
z <- data$medv[index]
sum(z) / length(z)
}
# Bootstrap calcuation for part b
set.seed(8)
boot.b <- boot(boston.data, u_medv.fn, R = 1000)
boot.b
u_medv.fn <- function(data, index) {
z <- data$medv[index]
sum(z) / length(z)
}
# Bootstrap calcuation for part b
set.seed(8)
boot.b <- boot(boston.data, u_medv.fn, R = 1000)
boot.b
boot.ci(boot.b, conf = 0.95, type="all")
boot.ci(boot.b, conf = 0.95, type=c("normal", "basic"))
boot.ci(boot.b, conf = 0.95, type="all")
22.53281 - 2*0.3990518
22.53281 + 2*0.3990518
boot.ci(boot.b, conf = 0.95, type="all")
t.test(boston.data$medv)
med_hat <- median(boston.data$medv)
med_hat
med_medv.fn <- function(data, index) {
median(data$medv[index])
}
boot.h <- boot(boston.data, med_medv.fn, 1000)
boot.h
med_medv.fn <- function(data, index) {
median(data$medv[index])
}
set.seed(10)
boot.h <- boot(boston.data, med_medv.fn, 1000)
boot.h
?quantile()
# Get tenth percentile of medv
u_10 <- quantile(boston.data$medv, probs = 0.1, na.rm = FALSE)
u_10
med_medv.fn <- function(data, index) {
median(data$medv[index])
}
set.seed(10)
boot.f <- boot(boston.data, med_medv.fn, 1000)
boot.f
ten_medv.fn <- function(data, index) {
quantile(data$medv, probs = 0.1, na.rm = FALSE)
}
set.seed(2)
boot.h <- boot(boston.data, ten_medv.fn, 1000)
boot.h
knitr::opts_chunk$set(echo = TRUE)
library(ISLR2)
library(MASS)
library(boot)
default.data <- Default
set.seed(1)
train.default <- sample(nrow(default.data), nrow(default.data) / 2)
glm.default <- glm(default ~ income + balance, data = default.data,
subset = train.default, family = binomial)
summary(glm.default)
boot.fn <- function(data, index)
+ coef(glm(default ~ income + balance, data = data,
subset = index, family = binomial))
boot.fn(default.data, train.default)
set.seed(9)
boot(default.data, boot.fn, 1000)
boston.data <- Boston
u_hat <- mean(boston.data$medv)
u_hat
se_hat <- sd(boston.data$medv) / sqrt(nrow(boston.data))
se_hat
u_medv.fn <- function(data, index) {
z <- data$medv[index]
sum(z) / length(z)
}
# Bootstrap calcuation for part b
set.seed(8)
boot.b <- boot(boston.data, u_medv.fn, R = 1000)
boot.b
# Bootstrap from part c estimates mean = 22.53281 and std err = 0.3990518
low_ci <- 22.53281 - 2*0.3990518
upper_ci <- 22.53281 + 2*0.3990518
# Use boot.ci() function to estimate confidence intervals
boot.ci(boot.b, conf = 0.95, type="all")
# Use t-test to estimate confidence intervals
t.test(boston.data$medv)
med_hat <- median(boston.data$medv)
med_hat
med_medv.fn <- function(data, index) {
median(data$medv[index])
}
set.seed(10)
boot.f <- boot(boston.data, med_medv.fn, 1000)
boot.f
# Get tenth percentile of medv
u_10 <- quantile(boston.data$medv, probs = 0.1, na.rm = FALSE)
u_10
ten_medv.fn <- function(data, index) {
quantile(data$medv, probs = 0.1, na.rm = FALSE)
}
set.seed(2)
boot.h <- boot(boston.data, ten_medv.fn, 1000)
boot.h
boot.fn <- function(data, index)
coef(glm(default ~ income + balance, data = data,
subset = index, family = binomial))
boot.fn(default.data, train.default)
set.seed(9)
boot(default.data, boot.fn, 1000)
knitr::opts_chunk$set(echo = TRUE)
library(ISLR2)
library(MASS)
install.packages(leaps)
library(leaps)
install.packages("leaps")
?sample()
# Load Dataset
college.data <- College
# Create vector half the size of college.data that contains random set of indices
set.seed(1)
train <- sample(nrow(college.data), nrow(college.data) / 2)
# Initialize training and test data
college.train <- college.data[train, ]
college.test <- college.datta[-train, ]
# Load Dataset
college.data <- College
# Create vector half the size of college.data that contains random set of indices
set.seed(1)
train <- sample(nrow(college.data), nrow(college.data) / 2)
# Initialize training and test data
college.train <- college.data[train, ]
college.test <- college.data[-train, ]
View(college.data)
?lm()
# Load Dataset
college.data <- College
# Create vector half the size of college.data that contains random set of indices
set.seed(1)
train <- sample(nrow(college.data), 0.8 * nrow(college.data))
# Initialize training and test data
college.train <- college.data[train, ]
college.test <- college.data[-train, ]
?predict()
lm.college <- lm(Apps ~ ., data = college.data, subset = train)
summary(lm.college)
lm.predict <- predict(lm.college, college.test)
lm.college <- lm(Apps ~ ., data = college.data, subset = train)
summary(lm.college)
lm.predict <- predict(lm.college, college.test)
mean((college.test$Apps - lm.predict)^2)
View(college.test)
lm.college <- lm(Apps ~ ., data = college.data, subset = train)
summary(lm.college)
lm.predict <- predict(lm.college, college.test)
lm.mse <- mean((college.test$Apps - lm.predict)^2)
lm.mse
install.packages("glmnet")
library(ISLR2)
library(MASS)
library(leaps)
library(glmnet)
knitr::opts_chunk$set(echo = TRUE)
library(ISLR2)
library(MASS)
library(leaps)
library(glmnet)
# Load Dataset
college.data <- College
college.data <- na.omit(college.data)
# Create vector half the size of college.data that contains random set of indices
set.seed(1)
train <- sample(nrow(college.data), 0.8 * nrow(college.data))
# Initialize training and test data
college.train <- college.data[train, ]
college.test <- college.data[-train, ]
lm.college <- lm(Apps ~ ., data = college.data, subset = train)
summary(lm.college)
lm.predict <- predict(lm.college, college.test)
lm.mse <- mean((college.test$Apps - lm.predict)^2)
lm.mse
View(college.data)
# Create matrix of x, the predictors, and vector of y, the response
x <- model.matrix(Apps ~ ., college.data)[, -2]
y <- college.data$Apps
# Create a lambda grid and use it to form ridge regression model
lambda.grid <- 10^seq(10, -2, length = 100)
ridge.mod <- glmnet(x[train, ], y[train], alpha = 0, lambda = grid, thresh = 1e-12)
# Create matrix of x, the predictors, and vector of y, the response
x <- model.matrix(Apps ~ ., college.data)[, -2]
y <- college.data$Apps
# Create a lambda grid and use it to form ridge regression model
lambda.grid <- 10^seq(10, -2, length = 100)
ridge.mod <- glmnet(x[train, ], y[train], alpha = 0, lambda = lambda.grid, thresh = 1e-12)
# Determine best lambda, or tuning parameter, using cross-validation
set.seed(2)
cv.out <- cv.glmnet(x[train, ], y[train], alpha = 0)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam
# Predict response of test data using ridge regression and calculate MSE
ridge.pred <- predict(ridge.mod, s = bestlam, newx = x[-train, ])
ridge.mse <- mean((ridge.pred - y[-train])^2)
# Create lasso model on the college dataset
lasso.mod <- glmnet(x[train, ], y[train], alpha = 1, lambda = lambda.grid)
plot(lasso.mod)
# Perform cross-validation to determine best lambda or tuning parameter
set.seed(3)
cv.out <- cv.glmnet(x[train, ], y[train], alpha = 1)
plot(cv.out)
bestlam2<- cv.out$lambda.min
# Predict response of test data and calculate MSE
lasso.pred <- predict(lasso.mod, s = bestlam2, newx = x[-train, ])
lasso.mse <- mean((lasso.pred - y[-train])^2)
lasso.mse
install.packages("pls")
library(ISLR2)
library(MASS)
library(leaps)
library(glmnet)
library(pls)
# Create PCR model on training data
set.seed(4)
pcr.fit <- pcr(Apps ~ ., data = college.data, subset = train, scale = T,
validation = "CV")
validationplot(pcr.fit, val.type = "MSEP")
?validationplot
# Create PCR model on training data
set.seed(4)
pcr.fit <- pcr(Apps ~ ., data = college.data, subset = train, scale = T,
validation = "CV")
validationplot(pcr.fit, val.type = "MSEP")
axis(side=1, at=seq(1, 20, by=1))
# Predict the number of applications using the PCR model
pcr.pred <- predict(pcr.fit, x[-train, ], ncomp = 5)
pcr.mse <- mean((pcr.pred - y[-train])^2)
pcr.mse
pcr.fit2 <- pcr(y ~ x, scale = T, ncomp = 5)
View(x)
# Create PLS model on the college data
set.seed(5)
pls.fit <- plsr(Apps ~ ., data = college.data, subset = train, scale = T,
validation = "CV")
summary(pls.fit)
validationplot(pls.fit, val.type = "MSEP")
axis(side=1, at=seq(1, 20, by=1))
# Predict number of applications using PLS
pls.pred <- predict(pls.fit, x[-train, ], ncomp = 7)
pls.mse <- mean((pls.pred - y[-train])^2)
pls.mse
mse.models <- data.frame(
model = c("least.squares", "ridge.regression", "lasso", "pcr", "pls"),
mse = c(lm.mse, ridge.mse, lasso.mse, pcr.mse, pls.mse),
stringsAsFactors = F
)
mse.models
boston.data <- Boston
boston.data <- na.omit(college.data)
# Create vector half the size of college.data that contains random set of indices
set.seed(7)
train <- sample(nrow(boston.data), 0.8 * nrow(boston.data))
test <- (!train)
boston.data <- Boston
boston.data <- na.omit(college.data)
# Create vector half the size of college.data that contains random set of indices
set.seed(7)
train <- sample(nrow(boston.data), 0.8 * nrow(boston.data))
test <- (-train)
boston.data <- Boston
boston.data <- na.omit(college.data)
# Create vector half the size of college.data that contains random set of indices
set.seed(7)
train <- sample(c(TRUE, FALSE), nrow(boston.data), replace = T)
test <- (!train)
View(boston.data)
boston.data <- Boston
boston.data <- na.omit(boston.data)
# Create vector half the size of college.data that contains random set of indices
set.seed(7)
train <- sample(c(TRUE, FALSE), nrow(boston.data), replace = T)
test <- (!train)
View(boston.data)
# Best Subset Selection
regfit.full <- regsubsets(crim ~ ., data = boston.data, nvmax = 13)
summary(regfitfull)
# Best Subset Selection
regfit.full <- regsubsets(crim ~ ., data = boston.data, nvmax = 13)
summary(regfit.full)
# Best Subset Selection
regfit.full <- regsubsets(crim ~ ., data = boston.data, nvmax = 13)
reg.summary <- summary(regfit.full)
par(mfrow = c(2,2))
plot(reg.summary$rss, xlab = "Number of Variables", ylab = "RSS", type = "l")
plot(reg.summary$adjr2, xlab = "Number of Variables", ylab = "Adjusted RSq",
type = "l")
par(mfrow = c(2,2))
plot(reg.summary$rss, xlab = "Number of Variables", ylab = "RSS", type = "l")
plot(reg.summary$adjr2, xlab = "Number of Variables", ylab = "Adjusted RSq",
type = "l")
which.max(reg$summaryadjr2)
par(mfrow = c(2,2))
plot(reg.summary$rss, xlab = "Number of Variables", ylab = "RSS", type = "l")
plot(reg.summary$adjr2, xlab = "Number of Variables", ylab = "Adjusted RSq",
type = "l")
which.max(reg.summary$adjr2)
par(mfrow = c(2,2))
plot(reg.summary$rss, xlab = "Number of Variables", ylab = "RSS", type = "l")
plot(reg.summary$adjr2, xlab = "Number of Variables", ylab = "Adjusted RSq",
type = "l")
which.max(reg.summary$adjr2)
points(9, reg.summary$adjr2[9], col = "red", cex = 2, pch = 20)
coeff(regfit.full, 9)
coef(regfit.full, 9)
# Ridge Regression
x2 <- model.matrix(crim ~ ., boston.data)[, -1]
y2 <- boston.data$crim
ridge.boston <- glmnet(x, y, alpha = 0, lambda = lambda.grid)
summary(ridge.boston)
# Ridge Regression
x2 <- model.matrix(crim ~ ., boston.data)[, -1]
y2 <- boston.data$crim
set.seed(8)
cv.out <- cv.glmnet(x, y, alpha = 0)
bestlam3 <- cv.out$lambda.min
out <- glmnet(x, y, alpha = 0)
predict(out, type = "coefficients", s = bestlam3)[1:13,]
# Ridge Regression
x2 <- model.matrix(crim ~ ., boston.data)[, -1]
y2 <- boston.data$crim
set.seed(8)
cv.out <- cv.glmnet(x2, y2, alpha = 0)
bestlam3 <- cv.out$lambda.min
out <- glmnet(x2, y2, alpha = 0)
predict(out, type = "coefficients", s = bestlam3)[1:13,]
# The Lasso
set.seed(9)
cv.out <- cv.glmnet(x2, y2, alpha = 1)
bestlam4 <- cv.out$lambda.min
out <- glmnet(x2, y2, alpha = 1, lambda = lambda.grid)
lasso.coef <- predict(out, type = "coefficients", s = bestlam4)[1:13,]
lasso.coef
# PCR
set.seed(11)
pcr.boston <- pcr(crim ~ ., data = boston.data, scale = T,
validation = "CV")
validationplot(pcr.boston, val.type = "MSEP")
# Fit PCR to entire data set using M = 8
pcr.boston <- pcr(y2 ~ x2, scale = T, ncomp = 5)
summary(pcr.boston)
# Use Validation-Set Approach to Determine Best Subset Selection Model
regfit.best <- regsubsets(crim ~ ., data = boston.data[train, ], nvmax = 13)
# Create test matrix
test.mat <- model.matrix(crim ~ ., data = boston.data[test, ])
# Compute test MSE for all possible amounts of variables used in the model
val.errors <- rep(NA, 13)
for (i in 1:13) {
coefi <- coef(regfit.best, id = i)
pred <- test.mat[, names(coefi)] %*% coefi
val.errors[i] <- mean((boston.data$crim[test] - pred)^2)
}
# Get coefficient estimates for model with best subset of variables
best.subset <- which.min(val.errors)
coef(regfit.best, best.subset)
# Predict the number of applications using the PCR model
pcr.pred <- predict(pcr.fit, x[-train, ], ncomp = 17)
pcr.mse <- mean((pcr.pred - y[-train])^2)
pcr.mse
mse.models <- data.frame(
model = c("least.squares", "ridge.regression", "lasso", "pcr", "pls"),
mse = c(lm.mse, ridge.mse, lasso.mse, pcr.mse, pls.mse),
stringsAsFactors = F
)
mse.models
setwd("C:/Users/janna/Documents/Merrimack MSDS/DSE6111/Final Project")
student.data <- read.csv("student-por.csv", na.strings = "?", stringsAsFactors = T)
View(student.data)
library(ISLR2)
library(MASS)
library(leaps)
library(glmnet)
library(pls)
library(boot)
d1=read.table("student-mat.csv",sep=";",header=TRUE)
d2=read.table("student-por.csv",sep=";",header=TRUE)
d3=merge(d1,d2,by=c("school","sex","age","address","famsize","Pstatus","Medu","Fedu","Mjob","Fjob","reason","nursery","internet"))
print(nrow(d3)) # 382 students
View(d2)
View(d3)
View(d3)
# Look for correlations between predictors and the response, G3
g3_cor <- cor(student.data, use="complete.obs")
liver <- read.table("bupa.data", encoding = "UTF-8")
View(liver)
liver <- read.table("bupa.data", sep=",", encoding = "UTF-8")
View(liver)
