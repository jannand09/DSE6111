nvmax = 11,
method = "forward")
for (i in 1:11) {
pred.forward <- predict.regsubsets(fstep.fit, white.quality[folds == j, ], id = i)
f.cv.errors[j, i] <-
mean((white.quality$quality[folds == j] - pred.forward)^2)
}
}
k <- 10
n <- nrow(white.quality)
set.seed(11)
folds <- sample(rep(1:k, length = n))
cv.errors <- matrix(NA, k, 11,
dimnames = list(NULL, paste(1:11)))
for (j in 1:k) {
fstep.fit <- regsubsets(quality ~ .,
data = white.quality[folds != j, ],
nvmax = 11,
method = "forward")
for (i in 1:11) {
pred.forward <- predict.regsubsets(fstep.fit, white.quality[folds == j, ], id = i)
f.cv.errors[j, i] <- mean((white.quality$quality[folds == j] - pred.forward)^2)
}
}
k <- 10
n <- nrow(white.quality)
set.seed(11)
folds <- sample(rep(1:k, length = n))
f.cv.errors <- matrix(NA, k, 11,
dimnames = list(NULL, paste(1:11)))
for (j in 1:k) {
fstep.fit <- regsubsets(quality ~ .,
data = white.quality[folds != j, ],
nvmax = 11,
method = "forward")
for (i in 1:11) {
pred.forward <- predict.regsubsets(fstep.fit, white.quality[folds == j, ], id = i)
f.cv.errors[j, i] <- mean((white.quality$quality[folds == j] - pred.forward)^2)
}
}
forward.cv.errors <- apply(f.cv.errors, 2, mean)
forward.cv.errors
par(mfrow = c(1,1))
plot(forward.cv.errors, type = "b")
k <- 10
n <- nrow(white.quality)
set.seed(11)
folds <- sample(rep(1:k, length = n))
b.cv.errors <- matrix(NA, k, 11,
dimnames = list(NULL, paste(1:11)))
for (j in 1:k) {
bstep.fit <- regsubsets(quality ~ .,
data = white.quality[folds != j, ],
nvmax = 11,
method = "backward")
for (i in 1:11) {
pred.backward <- predict.regsubsets(fstep.fit, white.quality[folds == j, ], id = i)
b.cv.errors[j, i] <-
mean((white.quality$quality[folds == j] - pred.backward)^2)
}
}
backward.cv.errors <- apply(b.cv.errors, 2, mean)
backward.cv.errors
par(mfrow = c(1,1))
plot(forward.cv.errors, type = "b")
plot(backward.cv.errors, type = "b")
backward.predictors <- which.min(backward.cv.errors)
backward.predictors
backward.mse <- backward.cv.errors[["11"]]
which.min(forward.cv.errors)
which.min(forward.cv.errors)
forward.mse <- forward.cv.errors[["11"]]
forward.mse
setwd("C:/Users/janna/Documents/Merrimack MSDS/DSE6111/Week 7")
install.packages("tree")
knitr::opts_chunk$set(echo = TRUE)
library(ISLR2)
library(tree)
# Split data into training and test data
set.seed(1)
train <- sample(1:nrow(Sales), nrow(Sales) / 2)
# Split data into training and test data
set.seed(1)
train <- sample(1:nrow(Carseats), nrow(Carseats) / 2)
tree.sales <- tree(Sales ~ ., data = Carseats, subset = train)
summary(tree.sales)
plot(tree.sales)
text(tree.sales, pretty = 0)
# Split data into training and test data
set.seed(1)
train <- sample(1:nrow(Carseats), nrow(Carseats) / 2)
sales.test <- Carseats[-train, "Sales"]
sales.pred <- predict(tree.sales, newdata = Carseats[-train, ])
plot(sales.pred, sales.test)
abline(0, 1)
mean((sales.pred - sales.test)^2)
cv.sales <- cv.tree(tree.sales)
plot(cv.sales$size, cv.sales$dev, type = "b")
?prune.tree
View(cv.sales)
?cv.tree
library(ISLR2)
library(tree)
library(MASS)
set.seed(1)
train <- sample(1:nrow(Boston), nrow(Boston) / 2)
boston.data <- Boston
boston.data <- Boston
tree.boston <- tree(medv ~ ., data = Boston, subset = train)
cv.boston <- cv.tree(tree.boston)
plot(cv.boston$size, cv.boston$dev, type = "b")
prune.sales <- prune.tree(tree.sales, best = 14)
plot(prune.sales)
text(prune.sales, pretty = 0)
prune.pred <- predict(prune.sales, newdata = Sales[-train, ])
prune.pred <- predict(prune.sales, newdata = Carseats[-train, ])
plot(prune.pred, sales.test)
prune.pred <- predict(prune.sales, newdata = Carseats[-train, ])
plot(prune.pred, sales.test)
View(Carseats)
dim(Carseats)
knitr::opts_chunk$set(echo = TRUE)
library(ISLR2)
library(MASS)
library(tree)
# Split data into training and test data
set.seed(1)
train <- sample(1:nrow(Carseats), nrow(Carseats) / 2)
sales.test <- Carseats[-train, "Sales"]
tree.sales <- tree(Sales ~ ., data = Carseats, subset = train)
summary(tree.sales)
plot(tree.sales)
text(tree.sales, pretty = 0)
sales.pred <- predict(tree.sales, newdata = Carseats[-train, ])
plot(sales.pred, sales.test)
abline(0, 1)
mean((sales.pred - sales.test)^2)
cv.sales <- cv.tree(tree.sales)
plot(cv.sales$size, cv.sales$dev, type = "b")
prune.sales <- prune.tree(tree.sales, best = 14)
plot(prune.sales)
text(prune.sales, pretty = 0)
prune.pred <- predict(prune.sales, newdata = Carseats[-train, ])
plot(prune.pred, sales.test)
mean((prune.pred - sales.test)^2)
install.packages("randomForest")
?ncol
library(randomForest)
set.seed(2)
bag.sales <- randomForest(Sales ~ ., data = Carseats, subset = train,
mtry = (ncol(Carseats) - 1), importance = T)
bag.pred <- predict(bag.sales, newdata = Carseats[-train, ])
plot(bag.pred, sales.test)
abline(0, 1)
mean((bag.pred - sales.test)^2)
set.seed(2)
# Create random forest model using default m = p/3
rf.sales <- randomForest(Sales ~ ., data = Carseats, subset = train,
importance = T)
rf.pred <- predict(rf.sales, newdata = Carseats[-train, ])
mean((rf.pred - sales.test)^2)
set.seed(2)
# Create random forest model using default m = 6
rf.sales <- randomForest(Sales ~ ., data = Carseats, subset = train,
mtry = 6, importance = T)
rf.pred <- predict(rf.sales, newdata = Carseats[-train, ])
mean((rf.pred - sales.test)^2)
set.seed(2)
# Create random forest model using default m = 9
rf.sales9 <- randomForest(Sales ~ ., data = Carseats, subset = train,
mtry = 9, importance = T)
rf.pred9 <- predict(rf.sales9, newdata = Carseats[-train, ])
mean((rf.pred9 - sales.test)^2)
set.seed(2)
# Create random forest model using default m = 6
rf.sales6 <- randomForest(Sales ~ ., data = Carseats, subset = train,
mtry = 6, importance = T)
rf.pred6 <- predict(rf.sales6, newdata = Carseats[-train, ])
mean((rf.pred6 - sales.test)^2)
set.seed(2)
# Create random forest model using default m = p/3
rf.sales <- randomForest(Sales ~ ., data = Carseats, subset = train,
importance = T)
rf.pred <- predict(rf.sales, newdata = Carseats[-train, ])
mean((rf.pred - sales.test)^2)
importance(rf.pred6)
importance(rf.sales6)
varImpPlot(rf.sales6)
install.packages("BART")
# Analyze data using BART
library(BART)
x <- Carseats[, 2:11]
y <- Carseats[, "Sales"]
xtrain <- x[train, ]
ytrain <- y[train]
xtest <- x[-train, ]
ytest <- y[-train]
# Run BART with default settings
set.seed(1)
bartfit <- gbart(xtrain, ytrain, x.test = xtest)
bart.pred <- bartfit$yhat.test.mean
mean((ytest - bart.pred)^2)
?OJ
# Divide OJ data set into training and test data
set.seed(3)
train.oj <- sample(1:nrow(OJ), 800)
test.oj <- OJ
# Divide OJ data set into training and test data
set.seed(3)
train.oj <- sample(1:nrow(OJ), 800)
test.oj <- OJ[-train.oj, ]
# Fit a classification tree to OJ data with Purchase as the response
tree.oj <- tree(Purchase ~ ., data = OJ, subset = train.oj)
summary(tree.oj)
tree.oj
tree.oj
plot(tree.oj)
plot(tree.oj, pretty = 0)
tree.oj
plot(tree.oj)
text(tree.oj, pretty = 0)
plot(tree.oj)
text(tree.oj, pretty = 0)
oj.pred <- predict(tree.oj, test.oj, type = "class")
table(oj.pred, test.oj)
oj.pred <- predict(tree.oj, test.oj, type = "class")
table(oj.pred, test.oj[-train, ])
oj.pred <- predict(tree.oj, test.oj, type = "class")
table(oj.pred, test.oj$Purchase)
(15 + 31) / (148 + 31 + 15 + 76)
set.seed(4)
cv.oj <- cv.tree(tree.oj, FUN = prune.misclass)
names(cv.oj)
cv.oj
plot(cv.oj$size, cv.oj$dev, type = "b")
prune.oj <- prune.misclass(tree.oj, best = 5)
plot(prune.oj)
text(prune.oj, pretty = 0)
prune.oj <- prune.misclass(tree.oj, best = 5)
plot(prune.oj)
text(prune.oj, pretty = 0)
prune.oj
prune.oj <- prune.misclass(tree.oj, best = 5)
plot(prune.oj)
text(prune.oj, pretty = 0)
summary(prune.oj)
# Predict response with pruned tree
prune.oj.pred <- predict(prune.oj, test.oj, type = "class")
# Generate confusion matrix for pruned tree
table(prune.oj.pred, test.oj$Purchase)
(15 + 31) / (148 + 31 + 15 + 76)
white.quality <- read.csv("winequality-white.csv", sep=";", na.strings = "?", stringsAsFactors = T)
setwd("C:/Users/janna/Documents/Merrimack MSDS/DSE6111/Final Project")
library(ISLR2)
library(MASS)
library(leaps)
library(glmnet)
library(pls)
library(boot)
library(trees)
library(ISLR2)
library(MASS)
library(leaps)
library(glmnet)
library(pls)
library(boot)
library(tree)
library(BART)
white.quality <- read.csv("winequality-white.csv", sep=";", na.strings = "?", stringsAsFactors = T)
View(white.quality)
set.seed(10)
train.white <- sample(1:nrow(white.quality), 0.5 * nrow(white.quality))
test.white <- (-train.white)
set.seed(2)
pls.fit <- plsr(quality ~ ., data = white.quality, subset = train.white, scale = T,
validation = "CV")
summary(pls.fit)
# Plot MSEP over the number of components
validationplot(pls.fit, val.type = "MSEP")
axis(side=1, at=seq(1, 20, by=1))
# Predict quality of the wine using PLS
pls.pred <- predict(pls.fit, x[test.white, ], ncomp=2)
x <- model.matrix(quality ~ ., white.quality)[, -12]
y <- white.quality$quality
y.test <- y[test.white]
set.seed(2)
pls.fit <- plsr(quality ~ ., data = white.quality, subset = train.white, scale = T,
validation = "CV")
summary(pls.fit)
# Plot MSEP over the number of components
validationplot(pls.fit, val.type = "MSEP")
axis(side=1, at=seq(1, 20, by=1))
# Predict quality of the wine using PLS
pls.pred <- predict(pls.fit, x[test.white, ], ncomp=2)
pls.mse <- mean((pls.pred - y.test)^2)
pls.mse
lm.train <- lm(quality ~ volatile.acidity + residual.sugar + free.sulfur.dioxide
+ density + pH + sulphates + alcohol, data = white.quality,
subset = train.white)
summary(lm.train)
lm.predict <- predict(lm.train, white.quality[test.white, ])
lm.mse <- mean((lm.predict - white.quality[test.white, ]$quality)^2)
lm.mse
?plsr()
View(pls.fit)
pls.fit$fitted.values
?predict
set.seed(2)
pls.fit <- plsr(quality ~ ., data = x, subset = train.white, scale = T,
validation = "CV")
set.seed(2)
pls.fit <- plsr(quality ~ ., data = white.quality, subset = train.white, scale = T,
validation = "CV")
# Predict quality of the wine using PLS
pls.pred <- predict(pls.fit, x[-train.white, ], ncomp=3)
pls.fit$fitted.values
View(x)
# Create matrix of x, the predictors, and vector of y, the response
x <- model.matrix(quality ~ ., white.quality)[, -13]
set.seed(2)
pls.fit <- plsr(quality ~ ., data = white.quality, subset = train.white, scale = T,
validation = "CV")
summary(pls.fit)
validationplot(pls.fit, val.type = "MSEP")
axis(side=1, at=seq(1, 20, by=1))
# Predict quality of the wine using PLS
pls.pred <- predict(pls.fit, newdata = x[test.white, ], ncomp=3)
lambda.grid <- 10^seq(10, -2, length = 100)
ridge.mod <- glmnet(x[train.white, ], y[train.white], alpha = 0, lambda = lambda.grid,
thresh = 1e-12)
# Determine the bets lambda, or tuning parameter, using cross-validation
set.seed(2)
cv.out <- cv.glmnet(x[train.white, ], y[train.white], alpha = 0)
plot(cv.out)
bestlam.ridge <- cv.out$lambda.min
bestlam.ridge
ridge.pred <- predict(ridge.mod, s = bestlam.ridge, newx = x[test.white, ])
ridge.mse <- mean((ridge.pred - y.test)^2)
ridge.mse
lasso.mod <- glmnet(x[train.white, ], y[train.white], alpha = 1, lambda = lambda.grid)
plot(lasso.mod)
# Perform cross-validation to determine best tuning parameter
set.seed(2)
cv.out <- cv.glmnet(x[train.white, ], y[train.white], alpha = 1)
plot(cv.out)
bestlam.lasso <- cv.out$lambda.min
bestlam.lasso
# Predict the response of test data and calculate MSE
lasso.pred <- predict(lasso.mod, s = bestlam.lasso, newx = x[test.white, ])
lasso.mse <- mean((lasso.pred - y.test)^2)
lasso.mse
set.seed(2)
pls.fit <- plsr(quality ~ ., data = white.quality, subset = train.white, scale = T,
validation = "CV")
summary(pls.fit)
validationplot(pls.fit, val.type = "MSEP")
axis(side=1, at=seq(1, 20, by=1))
pls.pred <- predict(pls.fit, newdata = x[test.white, ], ncomp=3)
ncol(x[test.white, ])
ncol(white.quality)
coef(pls.fit)
coef(lasso.mod)
View(lasso.mod)
?model.matrix
# Load libraries
library(ISLR2)
library(MASS)
library(leaps)
library(glmnet)
library(pls)
library(boot)
library(tree)
library(BART)
# Import wine quality data
white.quality <- read.csv("winequality-white.csv", sep=";", na.strings = "?", stringsAsFactors = T)
View(white.quality)
# Create training data
set.seed(10)
train.white <- sample(1:nrow(white.quality), 0.5 * nrow(white.quality))
test.white <- (-train.white)
# Create matrix of x, the predictors, and vector of y, the response
x <- model.matrix(quality ~ ., white.quality)[, -13]
y <- white.quality$quality
y.test <- y[test.white]
View(x)
# Create matrix of x, the predictors, and vector of y, the response
x <- model.matrix(quality ~ ., white.quality)[, -1]
set.seed(2)
pls.fit <- plsr(quality ~ ., data = white.quality, subset = train.white, scale = T,
validation = "CV")
summary(pls.fit)
validationplot(pls.fit, val.type = "MSEP")
axis(side=1, at=seq(1, 20, by=1))
# Predict quality of the wine using PLS
pls.pred <- predict(pls.fit, newdata = x[test.white, ], ncomp=3)
pls.mse <- mean((pls.pred - y.test)^2)
pls.mse
predict.regsubsets <- function(object, newdata, id, ...) {
form <- as.formula(object$call[[2]])
mat <- model.matrix(form, newdata)
coefi <- coef(object, id = id)
xvars <- names(coefi)
mat[, xvars] %*% coefi
}
k <- 10
n <- nrow(white.quality)
set.seed(11)
folds <- sample(rep(1:k, length = n))
f.cv.errors <- matrix(NA, k, 11,
dimnames = list(NULL, paste(1:11)))
for (j in 1:k) {
fstep.fit <- regsubsets(quality ~ .,
data = white.quality[folds != j, ],
nvmax = 11,
method = "forward")
for (i in 1:11) {
pred.forward <- predict.regsubsets(fstep.fit, white.quality[folds == j, ], id = i)
f.cv.errors[j, i] <- mean((white.quality$quality[folds == j] - pred.forward)^2)
}
}
forward.cv.errors <- apply(f.cv.errors, 2, mean)
forward.cv.errors
par(mfrow = c(1,1))
plot(forward.cv.errors, type = "b")
which.min(forward.cv.errors)
forward.mse <- forward.cv.errors[["8"]]
forward.mse
k <- 10
n <- nrow(white.quality)
set.seed(11)
folds <- sample(rep(1:k, length = n))
b.cv.errors <- matrix(NA, k, 11,
dimnames = list(NULL, paste(1:11)))
for (j in 1:k) {
bstep.fit <- regsubsets(quality ~ .,
data = white.quality[folds != j, ],
nvmax = 11,
method = "backward")
for (i in 1:11) {
pred.backward <- predict.regsubsets(fstep.fit, white.quality[folds == j, ], id = i)
b.cv.errors[j, i] <-
mean((white.quality$quality[folds == j] - pred.backward)^2)
}
}
backward.cv.errors <- apply(b.cv.errors, 2, mean)
backward.cv.errors
par(mfrow = c(1,1))
plot(backward.cv.errors, type = "b")
which.min(backward.cv.errors)
backward.mse <- backward.cv.errors[["11"]]
backward.mse
?apply
k <- 10
n <- nrow(white.quality)
set.seed(11)
folds <- sample(rep(1:k, length = n))
cv_sub.errors <- matrix(NA, k, 11,
dimnames = list(NULL, paste(1:11)))
for (j in 1:k) {
cv_sub.fit <- regsubsets(quality ~ .,
data = white.quality[folds != j, ],
nvmax = 11,
method = "forward")
for (i in 1:11) {
pred.cv_sub <- predict.regsubsets(cv_sub.fit, white.quality[folds == j, ], id = i)
cv.errors[j, i] <- mean((white.quality$quality[folds == j] - pred.cv_sub)^2)
}
}
k <- 10
n <- nrow(white.quality)
set.seed(11)
folds <- sample(rep(1:k, length = n))
cv_sub.errors <- matrix(NA, k, 11,
dimnames = list(NULL, paste(1:11)))
for (j in 1:k) {
cv_sub.fit <- regsubsets(quality ~ .,
data = white.quality[folds != j, ],
nvmax = 11,
method = "forward")
for (i in 1:11) {
pred.cv_sub <- predict.regsubsets(cv_sub.fit, white.quality[folds == j, ], id = i)
cv_sub.errors[j, i] <- mean((white.quality$quality[folds == j] - pred.cv_sub)^2)
}
}
cv_sub.cv.errors <- apply(cv_sub.errors, 2, mean)
?plot
plot(cv_sub.cv.errors, type = "b")
which.min(cv_sub.cv.errors)
cv_sub.mse <- cv_sub.cv.errors[["8"]]
cv_sub.mse
k <- 10
n <- nrow(white.quality)
set.seed(11)
folds <- sample(rep(1:k, length = n))
cv_sub.errors <- matrix(NA, k, 11,
dimnames = list(NULL, paste(1:11)))
for (j in 1:k) {
cv_sub.fit <- regsubsets(quality ~ .,
data = white.quality[folds != j, ],
nvmax = 11)
for (i in 1:11) {
pred.cv_sub <- predict.regsubsets(cv_sub.fit, white.quality[folds == j, ], id = i)
cv_sub.errors[j, i] <- mean((white.quality$quality[folds == j] - pred.cv_sub)^2)
}
}
cv_sub.cv.errors <- apply(cv_sub.errors, 2, mean)
par(mfrow(1,1))
par(mfrow = c(1,1))
plot(cv_sub.cv.errors, type = "b")
which.min(cv_sub.cv.errors)
cv_sub.mse <- cv_sub.cv.errors[["8"]]
cv_sub.mse
