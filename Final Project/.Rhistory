student.data <- na.omit(full.data)
student.data$Target[student.data$Target=="Enrolled"] <- "Student"
student.data$Target[student.data$Target=="Graduate"] <- "Student"
length(student.data$Target[student.data$Target == "Dropout"]) # 1421 dropout records
student.data$Target <- as.factor(student.data$Target)
model.errors <- data.frame(model = c("Log Regression", "LDA", "QDA", "NB Classifier",
"KNN", "Tree", "Pruned Tree", "Bagging", "RF",
"Boosting"),
test.error = rep(NA, 10))
# Split data into test and training set
set.seed(1)
train <- sample(1:nrow(student.data), 0.5 * nrow(student.data))
test <- (-train)
# Create logistic regression model using all predictors
log.student <- glm(Target ~ ., data = student.data, family = binomial)
summary(log.student)
sig_predictors <- c("Application.order", "Course", "Previous.qualification",
"Nacionality", "Mother.qualification", "Mother.occupation",
"Displaced", "Debtor", "Tuition.fees.up.to.date",
"Gender", "Scholarship.holder", "Age.at.enrollment", "International",
"Curricular.units.1st.sem.approved", "Curricular.units.1st.sem.grade",
"Curricular.units.2nd.sem.credited", "Curricular.units.2nd.sem.enrolled",
"Curricular.units.2nd.sem.approved", "Curricular.units.2nd.sem.grade",
"Unemployment.rate", "Target")
student.data <- student.data[, sig_predictors]
# Train logistic regression model with significant predictors
glm.student <- glm(Target ~ ., data = student.data.sig, subset = train,
family = binomial)
glm.student <- glm(Target ~ ., data = student.data, subset = train,
family = binomial)
glm.probs <- predict(glm.student, student.data[test, ], type = "response")
glm.pred <- rep("Student", nrow(student.data[test, ]))
attach(student.data)
contrasts(Target)
glm.pred[glm.probs < 0.5] <- "Dropout"
table(glm.pred, student.data$Target[test])
glm.error <- mean(glm.pred != student.data$Target[test])
glm.error
model.errors[model.errors$model == "Log Regression", "test.error"] <- glm.error
# Linear Discriminant Analysis
lda.student <- lda(Target ~ ., data = student.data, subset = train)
lda.student
lda.pred <- predict(lda.student, student.data[test, ])
lda.class <- lda.pred$class
table(lda.class, student.data$Target[test])
lda.error <- (216 + 60) / (499 + 60 + 216 + 1437)
lda.error
model.errors[model.errors$model == "LDA", "test.error"] <- lda.error
# Quadratic Discriminant Analysis
qda.student <- qda(Target ~ ., data = student.data, subset = train)
qda.student
qda.class <- predict(qda.student, student.data[test, ])$class
table(qda.class, student.data$Target[test])
qda.error <- (207 + 132) / (508 + 132 + 207 + 1365)
qda.error
model.errors[model.errors$model == "QDA", "test.error"] <- qda.error
# Naive Bayes Classifier
nb.student <- naiveBayes(Target ~ ., data = student.data, subset = train)
nb.student
nb.class <- predict(nb.student, student.data[test, ])
table(nb.class, student.data$Target[test])
nb.error <- (201 + 159) / (514 + 159 + 201 + 1338)
nb.error
model.errors[model.errors$model == "NB Classifier", "test.error"] <- nb.error
# K-Nearest Neighbor
length(sig_predictors)
train.X <- as.matrix(student.data[train, sig_predictors[-21]])
test.X <- as.matrix(student.data[test, sig_predictors[-21]])
train.target <- student.data$Target[train]
n.values <- c(1, 3, 5, 10)
knn.errors <- data.frame(n.value = n.values,
pred.error = rep(NA, length(n.values)))
for (x in 1:length(n.values)) {
set.seed(2)
knn.pred <- knn(train.X, test.X, train.target, k = n.values[x])
knn.errors[x, "pred.error"] <- mean(knn.pred != student.data$Target[test])
}
knn.errors # k = 5 lowest test error
model.errors[model.errors$model == "KNN", "test.error"] <- min(knn.errors$pred.error)
# Classification Tree Model
tree.student <- tree(Target ~ ., data = student.data, subset = train)
summary(tree.student)
plot(tree.student)
text(tree.student, pretty = 0)
tree.pred <- predict(tree.student, student.data[test, ], type = "class")
table(tree.pred, student.data$Target[test])
tree.error <- mean(tree.pred != student.data$Target[test])
tree.error
model.errors[model.errors$model == "Tree", "test.error"] <- tree.error
# Prune classification tree
set.seed(4)
cv.student <- cv.tree(tree.student, FUN = prune.misclass)
cv.student
plot(cv.student$size, cv.student$dev, type = "b")
pruned.student <- prune.misclass(tree.student, best = 7)
plot(pruned.student)
text(pruned.student, pretty = 0)
summary(pruned.student)
pruned.pred <- predict(pruned.student, student.data[test, ], type = "class")
table(pruned.pred, student.data$Target[test])
prune.error <- mean(pruned.pred != student.data$Target[test])
prune.error
model.errors[model.errors$model == "Pruned Tree", "test.error"] <- prune.error
# Bagging
set.seed(5)
bag.student <- randomForest(Target ~ ., data = student.data, subset = train,
mtry = length(sig_predictors)-1, importance = T)
bag.student
bag.pred <- predict(bag.student, student.data[test, ], type = "class")
table(bag.pred, student.data$Target[test])
bag.error <- mean(bag.pred != student.data$Target[test])
bag.error
model.errors[model.errors$model == "Bagging", "test.error"] <- bag.error
# Random Forest using default sqrt(p) predictors
set.seed(5)
rf.student <- randomForest(Target ~ ., data = student.data, subset = train,
importance = T)
rf.pred <- predict(rf.student, newdata = student.data[test, ], type = "class")
rf.error <- mean(rf.pred != student.data$Target[test])
rf.error
model.errors[model.errors$model == "RF", "test.error"] <- rf.error
# Boosting Model
tunings <- c(0.001, 0.01, 0.2, 0.5, 0.75, 1.0)
boost.results <- data.frame(shrinkage = tunings,
test.error = rep(NA, length(tunings)))
student.data$Target <- as.numeric(student.data$Target)
student.data$Target <- apply(student.data, 1, FUN = function(x) x[1] - 1)
for (x in 1:length(tunings)) {
set.seed(6)
boost.student <- gbm(Target ~ ., data = student.data[train, ],
distribution = "bernoulli", n.trees = 1000,
interaction.depth = 7, shrinkage = tunings[x])
boost.pred <- predict(boost.student, newdata = student.data[test, ],
type = "response", n.trees = 1000)
boost.results[x, "test.error"] <- mean(boost.pred != student.data$Target[test])
}
boost.results
model.errors[model.errors$model == "Boosting", "test.error"] <- min(boost.results$test.error)
attach(student.data)
contrasts(Target)
student.data <- student.data[, sig_predictors]
full.data <- read.table("student_data.csv", sep=";", header = T)
View(full.data)
student.data <- na.omit(full.data)
student.data$Target[student.data$Target=="Enrolled"] <- "Student"
student.data$Target[student.data$Target=="Graduate"] <- "Student"
student.data <- student.data[, sig_predictors]
student.data$Target <- as.factor(student.data$Target)
attach(student.data)
contrasts(Target)
View(lda.student)
varImpPlot(rf.student)
?varImpPlot
knitr::opts_chunk$set(echo = TRUE)
library(ISLR2)
library(MASS)
library(e1071)
library(class)
library(tree)
library(randomForest)
library(gbm)
full.data <- read.table("student_data.csv", sep=";", header = T)
View(full.data)
student.data <- na.omit(full.data)
student.data$Target[student.data$Target=="Enrolled"] <- "Student"
student.data$Target[student.data$Target=="Graduate"] <- "Student"
length(student.data$Target[student.data$Target == "Dropout"]) # 1421 dropout records
student.data$Target <- as.factor(student.data$Target)
set.seed(1)
train <- sample(1:nrow(student.data), 0.5 * nrow(student.data))
test <- (-train)
# Create data frame to track test errors of each model
model.errors <- data.frame(model = c("Log Regression", "LDA", "QDA", "NB Classifier",
"KNN", "Tree", "Pruned Tree", "Bagging", "RF",
"Boosting"),
test.error = rep(NA, 10))
log.student <- glm(Target ~ ., data = student.data, family = binomial)
summary(log.student)
student.data <- student.data[, sig_predictors]
knitr::opts_chunk$set(echo = TRUE)
library(ISLR2)
library(MASS)
library(e1071)
library(class)
library(tree)
library(randomForest)
library(gbm)
full.data <- read.table("student_data.csv", sep=";", header = T)
View(full.data)
student.data <- na.omit(full.data)
student.data$Target[student.data$Target=="Enrolled"] <- "Student"
student.data$Target[student.data$Target=="Graduate"] <- "Student"
length(student.data$Target[student.data$Target == "Dropout"]) # 1421 dropout records
student.data$Target <- as.factor(student.data$Target)
set.seed(1)
train <- sample(1:nrow(student.data), 0.5 * nrow(student.data))
test <- (-train)
# Create data frame to track test errors of each model
model.errors <- data.frame(model = c("Log Regression", "LDA", "QDA", "NB Classifier",
"KNN", "Tree", "Pruned Tree", "Bagging", "RF",
"Boosting"),
test.error = rep(NA, 10))
log.student <- glm(Target ~ ., data = student.data, family = binomial)
summary(log.student)
student.data <- student.data[, sig_predictors]
log.student <- glm(Target ~ ., data = student.data, family = binomial)
summary(log.student)
sig_predictors <- c("Application.order", "Course", "Previous.qualification",
"Nacionality", "Mother.qualification", "Mother.occupation",
"Displaced", "Debtor", "Tuition.fees.up.to.date",
"Gender", "Scholarship.holder", "Age.at.enrollment", "International",
"Curricular.units.1st.sem.approved", "Curricular.units.1st.sem.grade",
"Curricular.units.2nd.sem.credited", "Curricular.units.2nd.sem.enrolled",
"Curricular.units.2nd.sem.approved", "Curricular.units.2nd.sem.grade",
"Unemployment.rate", "Target")
student.data <- student.data[, sig_predictors]
glm.student <- glm(Target ~ ., data = student.data, subset = train,
family = binomial)
summary(glm.student)
glm.probs <- predict(glm.student, student.data[test, ], type = "response")
glm.pred <- rep("Student", nrow(student.data[test, ]))
attach(student.data)
contrasts(Target)
glm.pred[glm.probs < 0.5] <- "Dropout"
table(glm.pred, student.data$Target[test])
glm.error <- mean(glm.pred != student.data$Target[test])
glm.error
model.errors[model.errors$model == "Log Regression", "test.error"] <- glm.error
plot(glm.student)
lda.student <- lda(Target ~ ., data = student.data, subset = train)
lda.student
lda.pred <- predict(lda.student, student.data[test, ])
lda.class <- lda.pred$class
table(lda.class, student.data$Target[test])
lda.error <- (216 + 60) / (499 + 60 + 216 + 1437)
lda.error
model.errors[model.errors$model == "LDA", "test.error"] <- lda.error
qda.student <- qda(Target ~ ., data = student.data, subset = train)
qda.student
qda.class <- predict(qda.student, student.data[test, ])$class
table(qda.class, student.data$Target[test])
qda.error <- (207 + 132) / (508 + 132 + 207 + 1365)
qda.error
model.errors[model.errors$model == "QDA", "test.error"] <- qda.error
nb.student <- naiveBayes(Target ~ ., data = student.data, subset = train)
nb.student
nb.class <- predict(nb.student, student.data[test, ])
table(nb.class, student.data$Target[test])
nb.error <- (201 + 159) / (514 + 159 + 201 + 1338)
nb.error
model.errors[model.errors$model == "NB Classifier", "test.error"] <- nb.error
length(sig_predictors)
train.X <- as.matrix(student.data[train, sig_predictors[-21]])
test.X <- as.matrix(student.data[test, sig_predictors[-21]])
train.target <- student.data$Target[train]
n.values <- c(1, 3, 5, 10)
knn.errors <- data.frame(n.value = n.values,
pred.error = rep(NA, length(n.values)))
for (x in 1:length(n.values)) {
set.seed(2)
knn.pred <- knn(train.X, test.X, train.target, k = n.values[x])
knn.errors[x, "pred.error"] <- mean(knn.pred != student.data$Target[test])
}
knn.errors # k = 5 lowest test error
model.errors[model.errors$model == "KNN", "test.error"] <- min(knn.errors$pred.error)
# Classification Tree Model
tree.student <- tree(Target ~ ., data = student.data, subset = train)
summary(tree.student)
plot(tree.student)
text(tree.student, pretty = 0)
tree.pred <- predict(tree.student, student.data[test, ], type = "class")
table(tree.pred, student.data$Target[test])
tree.error <- mean(tree.pred != student.data$Target[test])
tree.error
model.errors[model.errors$model == "Tree", "test.error"] <- tree.error
set.seed(4)
cv.student <- cv.tree(tree.student, FUN = prune.misclass)
cv.student
plot(cv.student$size, cv.student$dev, type = "b")
pruned.student <- prune.misclass(tree.student, best = 7)
plot(pruned.student)
text(pruned.student, pretty = 0)
summary(pruned.student)
pruned.pred <- predict(pruned.student, student.data[test, ], type = "class")
table(pruned.pred, student.data$Target[test])
prune.error <- mean(pruned.pred != student.data$Target[test])
prune.error
model.errors[model.errors$model == "Pruned Tree", "test.error"] <- prune.error
set.seed(5)
bag.student <- randomForest(Target ~ ., data = student.data, subset = train,
mtry = length(sig_predictors)-1, importance = T)
bag.student
bag.pred <- predict(bag.student, student.data[test, ], type = "class")
table(bag.pred, student.data$Target[test])
bag.error <- mean(bag.pred != student.data$Target[test])
bag.error
model.errors[model.errors$model == "Bagging", "test.error"] <- bag.error
# Random Forest using default sqrt(p) predictors
set.seed(5)
rf.student <- randomForest(Target ~ ., data = student.data, subset = train,
importance = T)
rf.student
varImpPlot(rf.student)
rf.pred <- predict(rf.student, newdata = student.data[test, ], type = "class")
rf.error <- mean(rf.pred != student.data$Target[test])
rf.error
model.errors[model.errors$model == "RF", "test.error"] <- rf.error
tunings <- c(0.001, 0.01, 0.2, 0.5, 0.75, 1.0)
boost.results <- data.frame(shrinkage = tunings,
test.error = rep(NA, length(tunings)))
student.data$Target <- as.numeric(student.data$Target)
student.data$Target <- apply(student.data, 1, FUN = function(x) x[1] - 1)
for (x in 1:length(tunings)) {
set.seed(6)
boost.student <- gbm(Target ~ ., data = student.data[train, ],
distribution = "bernoulli", n.trees = 1000,
interaction.depth = 7, shrinkage = tunings[x])
boost.pred <- predict(boost.student, newdata = student.data[test, ],
type = "response", n.trees = 1000)
boost.results[x, "test.error"] <- mean(boost.pred != student.data$Target[test])
}
boost.results
model.errors[model.errors$model == "Boosting", "test.error"] <- min(boost.results$test.error)
model.errors
tunings <- c(0.001, 0.01, 0.2, 0.5, 0.75, 1.0)
boost.results <- data.frame(shrinkage = tunings,
test.error = rep(NA, length(tunings)))
student.data$Target <- as.numeric(student.data$Target)
student.data$Target <- apply(student.data, 1, FUN = function(x) x[1] - 1)
for (x in 1:length(tunings)) {
set.seed(6)
boost.student <- gbm(Target ~ ., data = student.data[train, ],
distribution = "bernoulli", n.trees = 1000,
interaction.depth = 4, shrinkage = tunings[x])
boost.pred <- predict(boost.student, newdata = student.data[test, ],
type = "response", n.trees = 1000)
boost.results[x, "test.error"] <- mean(boost.pred != student.data$Target[test])
}
boost.results
model.errors[model.errors$model == "Boosting", "test.error"] <- min(boost.results$test.error)
tunings <- c(0.001, 0.01, 0.2, 0.5, 0.75, 1.0)
boost.results <- data.frame(shrinkage = tunings,
test.error = rep(NA, length(tunings)))
student.data$Target <- as.numeric(student.data$Target)
student.data$Target <- apply(student.data, 1, FUN = function(x) x[1] - 1)
for (x in 1:length(tunings)) {
set.seed(6)
boost.student <- gbm(Target ~ ., data = student.data[train, ],
distribution = "bernoulli", n.trees = 1000,
interaction.depth = 10, shrinkage = tunings[x])
boost.pred <- predict(boost.student, newdata = student.data[test, ],
type = "response", n.trees = 1000)
boost.results[x, "test.error"] <- mean(boost.pred != student.data$Target[test])
}
boost.results
model.errors[model.errors$model == "Boosting", "test.error"] <- min(boost.results$test.error)
# Reinitialize student data after boosting changes
student.data <- na.omit(full.data)
student.data$Target[student.data$Target=="Enrolled"] <- "Student"
student.data$Target[student.data$Target=="Graduate"] <- "Student"
student.data$Target <- as.factor(student.data$Target)
student.data <- student.data[, sig_predictors]
log.final <- glm(Target ~ Curricular.units.2nd.sem.approved + Tuition.fees.up.to.date
+ Age.at.enrollment + Curricular.units.2nd.sem.enrolled, data = student.data,
subset = train, family = binomial)
summary(log.final)
log.probs <- predict(log.final, student.data[test, ], type = "response")
log.pred <- rep("Student", nrow(student.data[test, ]))
contrasts(student.data$Target)
log.pred[glm.probs < 0.5] <- "Dropout"
table(log.pred, student.data$Target[test])
log.error <- mean(log.pred != student.data$Target[test])
log.error
knitr::opts_chunk$set(echo = TRUE)
library(ISLR2)
library(MASS)
library(leaps)
library(glmnet)
library(pls)
library(boot)
library(tree)
library(randomForest)
library(gbm)
library(BART)
# Import wine quality data
wine.data <- read.csv("winequality-white.csv", sep=";", na.strings = "?", stringsAsFactors = T)
View(wine.data)
white_cor <- cor(wine.data, use="complete.obs")
print(white_cor[12, ])
lm.wine <- lm(quality ~ ., data = wine.data)
summary(lm.wine)
# Create training and test data
set.seed(10)
train.wine <- sample(1:nrow(wine.data), 0.5 * nrow(wine.data))
test.wine <- (-train.wine)
# Train a linear model with statistically significant predictors
lm.train <- lm(quality ~ volatile.acidity + residual.sugar + free.sulfur.dioxide
+ density + pH + sulphates + alcohol, data = wine.data,
subset = train.wine)
summary(lm.train)
# Use trained linear model to predict the wine quality
lm.predict <- predict(lm.train, wine.data[test.wine, ])
lm.mse <- mean((lm.predict - wine.data$quality[test.wine])^2)
lm.mse
# Train the first linear model
first.lm.wine <- lm(quality ~ ., data = wine.data, subset = train.wine)
summary(first.lm.wine)
# Use first linear model to predict the wine quality
first.lm.predict <- predict(first.lm.wine, wine.data[test.wine, ])
first.lm.mse <- mean((first.lm.predict - wine.data$quality[test.wine])^2)
first.lm.mse
# Create matrix of x, the predictors, and vector of y, the response
x <- model.matrix(quality ~ ., wine.data)[, -1]
y <- wine.data$quality
y.test <- y[test.wine]
# Create a lambda grid and use it to form ridge regression model
lambda.grid <- 10^seq(10, -2, length = 100)
ridge.mod <- glmnet(x[train.wine, ], y[train.wine], alpha = 0, lambda = lambda.grid,
thresh = 1e-12)
ridge.mod
# Predict the response of test data and calculate MSE
lasso.pred <- predict(lasso.mod, s = bestlam.lasso, newx = x[test.wine, ])
knitr::opts_chunk$set(echo = TRUE)
library(ISLR2)
library(MASS)
library(leaps)
library(glmnet)
library(pls)
library(boot)
library(tree)
library(randomForest)
library(gbm)
library(BART)
# Import wine quality data
wine.data <- read.csv("winequality-white.csv", sep=";", na.strings = "?", stringsAsFactors = T)
View(wine.data)
white_cor <- cor(wine.data, use="complete.obs")
print(white_cor[12, ])
lm.wine <- lm(quality ~ ., data = wine.data)
summary(lm.wine)
# Create training and test data
set.seed(10)
train.wine <- sample(1:nrow(wine.data), 0.5 * nrow(wine.data))
test.wine <- (-train.wine)
# Train a linear model with statistically significant predictors
lm.train <- lm(quality ~ volatile.acidity + residual.sugar + free.sulfur.dioxide
+ density + pH + sulphates + alcohol, data = wine.data,
subset = train.wine)
summary(lm.train)
# Use trained linear model to predict the wine quality
lm.predict <- predict(lm.train, wine.data[test.wine, ])
lm.mse <- mean((lm.predict - wine.data$quality[test.wine])^2)
lm.mse
# Train the first linear model
first.lm.wine <- lm(quality ~ ., data = wine.data, subset = train.wine)
summary(first.lm.wine)
# Use first linear model to predict the wine quality
first.lm.predict <- predict(first.lm.wine, wine.data[test.wine, ])
first.lm.mse <- mean((first.lm.predict - wine.data$quality[test.wine])^2)
first.lm.mse
# Create matrix of x, the predictors, and vector of y, the response
x <- model.matrix(quality ~ ., wine.data)[, -1]
y <- wine.data$quality
y.test <- y[test.wine]
# Create a lambda grid and use it to form ridge regression model
lambda.grid <- 10^seq(10, -2, length = 100)
ridge.mod <- glmnet(x[train.wine, ], y[train.wine], alpha = 0, lambda = lambda.grid,
thresh = 1e-12)
ridge.mod
# Determine the best lambda, or tuning parameter, using cross-validation
set.seed(2)
cv.out <- cv.glmnet(x[train.wine, ], y[train.wine], alpha = 0)
plot(cv.out)
bestlam.ridge <- cv.out$lambda.min
bestlam.ridge
# Predict the response of test data using ridge regression with best tuning parameter
ridge.pred <- predict(ridge.mod, s = bestlam.ridge, newx = x[test.wine, ])
ridge.mse <- mean((ridge.pred - y.test)^2)
ridge.mse
lasso.mod <- glmnet(x[train.wine, ], y[train.wine], alpha = 1, lambda = lambda.grid)
plot(lasso.mod)
# Perform cross-validation to determine best tuning parameter
set.seed(2)
cv.out <- cv.glmnet(x[train.wine, ], y[train.wine], alpha = 1)
plot(cv.out)
bestlam.lasso <- cv.out$lambda.min
bestlam.lasso
# Predict the response of test data and calculate MSE
lasso.pred <- predict(lasso.mod, s = bestlam.lasso, newx = x[test.wine, ])
lasso.mse <- mean((lasso.pred - y.test)^2)
lasso.mse
View(lasso.mod)
# Create PLS model on the wine wine quality data
set.seed(2)
pls.fit <- plsr(quality ~ ., data = wine.data, subset = train.wine, scale = T,
validation = "CV")
summary(pls.fit)
# Plot MSEP over the number of components
validationplot(pls.fit, val.type = "MSEP")
axis(side=1, at=seq(1, 20, by=1))
# Predict quality of the wine using PLS
pls.pred <- predict(pls.fit, newdata = x[test.wine, ], ncomp=3)
pls.mse <- mean((pls.pred - y.test)^2)
pls.mse
lasso.coef <- predict(lasso.mod, type = "coefficients", s = bestlam.lasso)[1:11, ]
lasso.coef[lasso.coef != 0]
lasso.coef <- predict(lasso.mod, type = "coefficients", s = bestlam.lasso)[1:11, ]
lasso.coef
lasso.coef[lasso.coef != 0]
set.seed(90)
rf.wine <- randomForest(quality ~ ., data = wine.data[train.wine, ])
rf.wine
