set.seed(8)
cv.out <- cv.glmnet(x2, y2, alpha = 0)
bestlam3 <- cv.out$lambda.min
out <- glmnet(x2, y2, alpha = 0)
predict(out, type = "coefficients", s = bestlam3)[1:13,]
# The Lasso
set.seed(9)
cv.out <- cv.glmnet(x2, y2, alpha = 1)
bestlam4 <- cv.out$lambda.min
out <- glmnet(x2, y2, alpha = 1, lambda = lambda.grid)
lasso.coef <- predict(out, type = "coefficients", s = bestlam4)[1:13,]
lasso.coef
# PCR
set.seed(11)
pcr.boston <- pcr(crim ~ ., data = boston.data, scale = T,
validation = "CV")
validationplot(pcr.boston, val.type = "MSEP")
# Fit PCR to entire data set using M = 8
pcr.boston <- pcr(y2 ~ x2, scale = T, ncomp = 5)
summary(pcr.boston)
# Use Validation-Set Approach to Determine Best Subset Selection Model
regfit.best <- regsubsets(crim ~ ., data = boston.data[train, ], nvmax = 13)
# Create test matrix
test.mat <- model.matrix(crim ~ ., data = boston.data[test, ])
# Compute test MSE for all possible amounts of variables used in the model
val.errors <- rep(NA, 13)
for (i in 1:13) {
coefi <- coef(regfit.best, id = i)
pred <- test.mat[, names(coefi)] %*% coefi
val.errors[i] <- mean((boston.data$crim[test] - pred)^2)
}
# Get coefficient estimates for model with best subset of variables
best.subset <- which.min(val.errors)
coef(regfit.best, best.subset)
# Predict the number of applications using the PCR model
pcr.pred <- predict(pcr.fit, x[-train, ], ncomp = 17)
pcr.mse <- mean((pcr.pred - y[-train])^2)
pcr.mse
mse.models <- data.frame(
model = c("least.squares", "ridge.regression", "lasso", "pcr", "pls"),
mse = c(lm.mse, ridge.mse, lasso.mse, pcr.mse, pls.mse),
stringsAsFactors = F
)
mse.models
knitr::opts_chunk$set(echo = TRUE)
library(ISLR2)
library(MASS)
library(tree)
# Split data into training and test data
set.seed(1)
train <- sample(1:nrow(Carseats), nrow(Carseats) / 2)
sales.test <- Carseats[-train, "Sales"]
tree.sales <- tree(Sales ~ ., data = Carseats, subset = train)
summary(tree.sales)
plot(tree.sales)
text(tree.sales, pretty = 0)
sales.pred <- predict(tree.sales, newdata = Carseats[-train, ])
plot(sales.pred, sales.test)
abline(0, 1)
mean((sales.pred - sales.test)^2)
cv.sales <- cv.tree(tree.sales)
plot(cv.sales$size, cv.sales$dev, type = "b")
prune.sales <- prune.tree(tree.sales, best = 14)
plot(prune.sales)
text(prune.sales, pretty = 0)
prune.pred <- predict(prune.sales, newdata = Carseats[-train, ])
plot(prune.pred, sales.test)
mean((prune.pred - sales.test)^2)
library(randomForest)
set.seed(2)
bag.sales <- randomForest(Sales ~ ., data = Carseats, subset = train,
mtry = (ncol(Carseats) - 1), importance = T)
bag.sales
bag.pred <- predict(bag.sales, newdata = Carseats[-train, ])
plot(bag.pred, sales.test)
abline(0, 1)
mean((bag.pred - sales.test)^2)
set.seed(2)
# Create random forest model using default m = p/3
rf.sales <- randomForest(Sales ~ ., data = Carseats, subset = train,
importance = T)
rf.pred <- predict(rf.sales, newdata = Carseats[-train, ])
mean((rf.pred - sales.test)^2)
set.seed(2)
# Create random forest model using default m = 6
rf.sales6 <- randomForest(Sales ~ ., data = Carseats, subset = train,
mtry = 6, importance = T)
rf.pred6 <- predict(rf.sales6, newdata = Carseats[-train, ])
mean((rf.pred6 - sales.test)^2)
set.seed(2)
# Create random forest model using default m = 9
rf.sales9 <- randomForest(Sales ~ ., data = Carseats, subset = train,
mtry = 9, importance = T)
rf.pred9 <- predict(rf.sales9, newdata = Carseats[-train, ])
mean((rf.pred9 - sales.test)^2)
importance(rf.sales6)
varImpPlot(rf.sales6)
# Analyze data using BART
library(BART)
x <- Carseats[, 2:11]
y <- Carseats[, "Sales"]
xtrain <- x[train, ]
ytrain <- y[train]
xtest <- x[-train, ]
ytest <- y[-train]
# Run BART with default settings
set.seed(1)
bartfit <- gbart(xtrain, ytrain, x.test = xtest)
bart.pred <- bartfit$yhat.test.mean
mean((ytest - bart.pred)^2)
# Divide OJ data set into training and test data
set.seed(3)
train.oj <- sample(1:nrow(OJ), 800)
test.oj <- OJ[-train.oj, ]
# Fit a classification tree to OJ data with Purchase as the response
tree.oj <- tree(Purchase ~ ., data = OJ, subset = train.oj)
summary(tree.oj)
tree.oj
plot(tree.oj)
text(tree.oj, pretty = 0)
# Use classification tree to predict responses of test data
oj.pred <- predict(tree.oj, test.oj, type = "class")
# Generate confusion matrix
table(oj.pred, test.oj$Purchase)
(15 + 31) / (148 + 31 + 15 + 76)
# Use cross-validation to determine if pruning the tree may result in better prediction error
set.seed(4)
cv.oj <- cv.tree(tree.oj, FUN = prune.misclass)
names(cv.oj)
cv.oj
# Create plot of cross-validated training error over tree size
plot(cv.oj$size, cv.oj$dev, type = "b")
prune.oj <- prune.misclass(tree.oj, best = 5)
plot(prune.oj)
text(prune.oj, pretty = 0)
summary(prune.oj)
# Predict response with pruned tree
prune.oj.pred <- predict(prune.oj, test.oj, type = "class")
# Generate confusion matrix for pruned tree
table(prune.oj.pred, test.oj$Purchase)
(15 + 31) / (148 + 31 + 15 + 76)
?na.omit
hitters.data <- na.omit(Hitters)
View(hitters.data)
hitters.train <- c(1:200)
hitters.data <- na.omit(Hitters)
hitters.data$Salary <- log(hitters.data$Salary)
View(hitters.data)
hitters.train <- c(1:200)
hitters.test <- c(201:nrow(hitters.data))
install.packages("gbm")
library(gbm)
set.seed(5)
boost.hitters <- gbm(Salary ~ ., data = hitters.data[hitters.train, ],
distribution = "gaussian", n.trees = 1000,
interaction.depth = 4)
View(boost.hitters)
?rep
?gbm
library(gbm)
set.seed(5)
tunings <- c(0.001, 0.01, 0.2, 0.5, 1.0)
gbm.training <- data.frame(lambda = tunings,
training.error = rep(NA, 5))
for (x in length(tunings)) {
boost.hitters <- gbm(Salary ~ ., data = hitters.data[hitters.train, ],
distribution = "gaussian", n.trees = 1000,
interaction.depth = 4, shrinkage = tunings[x])
gbm.training[x, "training.error"] <-  mean(boost.hitters$train.error)
}
gbm.training
library(gbm)
set.seed(5)
tunings <- c(0.001, 0.01, 0.2, 0.5, 1.0)
gbm.training <- data.frame(lambda = tunings,
training.error = rep(NA, 5))
for (x in 1:length(tunings)) {
boost.hitters <- gbm(Salary ~ ., data = hitters.data[hitters.train, ],
distribution = "gaussian", n.trees = 1000,
interaction.depth = 4, shrinkage = tunings[x])
gbm.training[x, "training.error"] <-  mean(boost.hitters$train.error)
}
gbm.training
library(gbm)
set.seed(5)
tunings <- c(0.001, 0.01, 0.2, 0.5, 0.75, 1.0)
gbm.training <- data.frame(lambda = tunings,
training.error = rep(NA, length(tunings)))
for (x in 1:length(tunings)) {
boost.hitters <- gbm(Salary ~ ., data = hitters.data[hitters.train, ],
distribution = "gaussian", n.trees = 1000,
interaction.depth = 4, shrinkage = tunings[x])
gbm.training[x, "training.error"] <-  mean(boost.hitters$train.error)
}
gbm.training
plot(gbm.training$lambda, gbm.training$training.error)
plot(gbm.training$lambda, gbm.training$training.error, type = "b")
gbm.test <- data.frame(lambda = tunings,
test.error = rep(NA, length(tunings)))
for (x in 1:length(tunings)) {
boost.hitters <- gbm(Salary ~ ., data = hitters.data[hitters.train, ],
distribution = "gaussian", n.trees = 1000,
interaction.depth = 4, shrinkage = tunings[x])
yhat.boost <- predict(boost.hitters, newdata = hitters.data[hitters.test, ],
n.trees = 1000)
gbm.test[x, "test.error"] <-  mean((yhat.boost - hitters.data$Salary[hitters.test])^2)
}
gbm.test
plot(gbm.test$lambda, gbm.test$test.error, type = "b")
# Create LS regression with Salary as the predictor and determine test MSE
lm.hitters <- lm(Salary ~ ., data = hitters.data, subset = hitters.train)
lm.predict <- predict(lm.hitters, newdata = hitters.data[hitters.test, ])
lm.mse <- mean((lm.predict - hitters.data$Salary[hitters.test])^2)
lm.mse
?seq
x <- model.matrix(Salary ~ ., hitters.data)[-1]
y <- hitters.data$Salary
lambda.grid <- 10^seq(10, -2, length = 100)
lasso.mod <- glmnet(x[hitters.train, ], y[hitters.train], alpha = 1,
lambda = lambda.grid)
library(glmnet)
x <- model.matrix(Salary ~ ., hitters.data)[-1]
y <- hitters.data$Salary
lambda.grid <- 10^seq(10, -2, length = 100)
lasso.mod <- glmnet(x[hitters.train, ], y[hitters.train], alpha = 1,
lambda = lambda.grid)
?College
dim(x[hitters.train, ])
library(glmnet)
x <- model.matrix(Salary ~ ., hitters.data)[, -1]
y <- hitters.data$Salary
lambda.grid <- 10^seq(10, -2, length = 100)
lasso.mod <- glmnet(x[hitters.train, ], y[hitters.train], alpha = 1,
lambda = lambda.grid)
set.seed(7)
cv.out <- cv.glmnet(x[hitters.train, ], y[hitters.train], alpha = 1)
bestlam <- cv.out$lambda.min
lasso.pred <- predict(lasso.mod, s = bestlam, newx = x[hitters.test, ])
lasso.mse <- mean((lasso.pred - y[hitters.test])^2)
lasso.mse
# Create boosting model using shrinkage, or tuning parameter, equal to 0.01
boost.hitters <- gbm(Salary ~ ., data = hitters.data[hitters.train, ],
distribution = "gaussian", n.trees = 1000,
interaction.depth = 4, shrinkage = 0.01)
summary(boost.hitters)
?Hitters
gbm.test <- data.frame(lambda = tunings,
test.error = rep(NA, length(tunings)))
for (x in 1:length(tunings)) {
set.seed(5)
boost.hitters <- gbm(Salary ~ ., data = hitters.data[hitters.train, ],
distribution = "gaussian", n.trees = 1000,
interaction.depth = 4, shrinkage = tunings[x])
yhat.boost <- predict(boost.hitters, newdata = hitters.data[hitters.test, ],
n.trees = 1000)
gbm.test[x, "test.error"] <-  mean((yhat.boost - hitters.data$Salary[hitters.test])^2)
}
gbm.test
plot(gbm.test$lambda, gbm.test$test.error, type = "b")
set.seed(5)
bag.hitters <- randomForest(Salary ~ ., data = hitters.data,
subset = hitters.train, mtry = 19,
importance = T)
yhat.bag <- predict(bag.hitters, newdata = hitters.data[hitters.test, ])
mean((yhat.bag - hitters.data$Salary[hitters.test])^2)
knitr::opts_chunk$set(echo = TRUE)
# Create PLS model on the wine wine quality data
set.seed(2)
pls.fit <- plsr(quality ~ ., data = wine.data, subset = train.wine, scale = T,
validation = "CV")
library(ISLR2)
library(MASS)
library(leaps)
library(glmnet)
library(pls)
library(boot)
library(tree)
library(randomForest)
library(gbm)
library(BART)
# Import wine quality data
wine.data <- read.csv("winequality-white.csv", sep=";", na.strings = "?", stringsAsFactors = T)
View(wine.data)
# Create training and test data
set.seed(10)
train.wine <- sample(1:nrow(wine.data), 0.5 * nrow(wine.data))
test.wine <- (-train.wine)
# Create matrix of x, the predictors, and vector of y, the response
x <- model.matrix(quality ~ ., wine.data)[, -1]
y <- wine.data$quality
y.test <- y[test.wine]
# Create PLS model on the wine wine quality data
set.seed(2)
pls.fit <- plsr(quality ~ ., data = wine.data, subset = train.wine, scale = T,
validation = "CV")
summary(pls.fit)
# Plot MSEP over the number of components
validationplot(pls.fit, val.type = "MSEP")
axis(side=1, at=seq(1, 20, by=1))
setwd("C:/Users/janna/Documents/Merrimack MSDS/DSE6111/Final Project")
# Load Libraries
library(e1071)
library(class)
# Load data set
full.data <- read.table("student_data.csv", sep=";", header = T, stringsAsFactors = T)
View(full.data)
student.data <- na.omit(full.data)
# Split data into test and training set
set.seed(1)
train <- sample(1:nrow(student.data), 0.5 * nrow(student.data))
test <- (-train)
# Create logistic regression model using all predictors
log.student <- glm(Target ~ ., data = student.data, family = binomial)
summary(log.student)
sig_predictors <- c("Application.order", "Course", "Previous.qualification",
"Nacionality", "Mother.qualification", "Mother.occupation",
"Displaced", "Debtor", "Tuition.fees.up.to.date",
"Gender", "Scholarship.holder", "Age.at.enrollment", "International",
"Curricular.units.1st.sem.approved", "Curricular.units.1st.sem.grade",
"Curricular.units.2nd.sem.credited", "Curricular.units.2nd.sem.enrolled",
"Curricular.units.2nd.sem.approved", "Curricular.units.2nd.sem.grade",
"Unemployment.rate", "Target")
student.data <- student.data[, sig_predictors]
# Train logistic regression model with significant predictors
glm.student <- glm(Target ~ ., data = student.data, subset = train,
family = binomial)
glm.probs <- predict(glm.student, student.data[test, ], type = "response")
glm.pred <- rep("Graduate", nrow(student.data[test, ]))
contrasts(Target)
attach(student.data)
contrasts(Target)
student.data["Target"][student.data["Target"] == "Enrolled" | student.data["Target" == "Graduate"]] <- "Student"
student.data$Target[student.data$Target=="Enrolled"] <- "Student"
View(student.data)
full.data <- read.table("student_data.csv", sep=";", header = T)
student.data <- na.omit(full.data)
student.data$Target[student.data$Target=="Enrolled"] <- "Student"
student.data$Target[student.data$Target=="Graduate"] <- "Student"
student.data$Target <- as.factor(student.data$Target)
set.seed(1)
train <- sample(1:nrow(student.data), 0.5 * nrow(student.data))
test <- (-train)
# Create logistic regression model using all predictors
log.student <- glm(Target ~ ., data = student.data, family = binomial)
summary(log.student)
attach(student.data)
contrasts(Target)
full.data <- read.table("student_data.csv", sep=";", header = T)
student.data <- na.omit(full.data)
student.data$Target[student.data$Target=="Enrolled"] <- "Student"
student.data$Target[student.data$Target=="Graduate"] <- "Student"
length(student.data$Target[student.data$Target == "Dropout"])
student.data$Target <- as.factor(student.data$Target)
# Split data into test and training set
set.seed(1)
train <- sample(1:nrow(student.data), 0.5 * nrow(student.data))
test <- (-train)
# Create logistic regression model using all predictors
log.student <- glm(Target ~ ., data = student.data, family = binomial)
summary(log.student)
sig_predictors <- c("Application.order", "Course", "Previous.qualification",
"Nacionality", "Mother.qualification", "Mother.occupation",
"Displaced", "Debtor", "Tuition.fees.up.to.date",
"Gender", "Scholarship.holder", "Age.at.enrollment", "International",
"Curricular.units.1st.sem.approved", "Curricular.units.1st.sem.grade",
"Curricular.units.2nd.sem.credited", "Curricular.units.2nd.sem.enrolled",
"Curricular.units.2nd.sem.approved", "Curricular.units.2nd.sem.grade",
"Unemployment.rate", "Target")
student.data <- student.data[, sig_predictors]
# Train logistic regression model with significant predictors
glm.student <- glm(Target ~ ., data = student.data, subset = train,
family = binomial)
glm.probs <- predict(glm.student, student.data[test, ], type = "response")
glm.pred <- rep("Student", nrow(student.data[test, ]))
attach(student.data)
contrasts(Target)
glm.pred[glm.probs < 0.5] <- "Dropout"
table(glm.pred, student.data$Target[test])
mean(glm.probs == student.data$Target[test])
glm.error <- (199 + 75) / (199 + 75 + 516 + 1422)
glm.error
lda.student <- lda(Target ~ ., data = student.data, subset = train)
library(e1071)
library(class)
lda.student <- lda(Target ~ ., data = student.data, subset = train)
library(ISLR2)
library(MASS)
lda.student <- lda(Target ~ ., data = student.data, subset = train)
lda.student
lda.pred <- predict(lda.student, student.data[test, ])
lda.class <- lda.pred$class
table(lda.class, student.data$Target[test])
lda.error <- (216 + 60) / (499 + 60 + 216 + 1437)
lda.error
qda.student <- qda(Target ~ ., data = student.data, subset = train)
qda.student
qda.class <- predict(qda.student, student.data[test, ])$class
table(qda.class, student.data$Target[test])
qda.error <- (207 + 132) / (508 + 132 + 207 + 1365)
qda.error
nb.student <- naiveBayes(Target ~ ., data = student.data, subset = train)
nb.student
nb.class <- predict(nb.student, student.data[test, ])
table(nb.class, student.data$Target[test])
nb.error <- (201 + 159) / (514 + 159 + 201 + 1338)
nb.error
train.X <- as.matrix(student.data[train, sig_predictors])
test.X <- as.matrix(student.data[test, sig_predictors])
train.target <- student.data$Target[train]
n.values <- c(1, 3, 5, 10)
knn.errors <- data.frame(n.value <- n.values,
pred.error <- rep(NA, length(n.values)))
for (x in 1:length(n.values)) {
set.seed(2)
knn.pred <- knn(train.X, test.X, train.target, k = n.values[x])
mean(knn.pred == student.data$Target[test])
}
knn.pred <- knn(train.X, test.X, train.target, k = 1)
View(test.X)
full.data <- read.table("student_data.csv", sep=";", header = T)
View(full.data)
student.data <- na.omit(full.data)
student.data$Target[student.data$Target=="Enrolled"] <- "Student"
student.data$Target[student.data$Target=="Graduate"] <- "Student"
length(student.data$Target[student.data$Target == "Dropout"]) # 1421 dropout records
student.data$Target <- as.factor(student.data$Target)
# Split data into test and training set
set.seed(1)
train <- sample(1:nrow(student.data), 0.5 * nrow(student.data))
test <- (-train)
student.data <- student.data[, sig_predictors]
glm.student <- glm(Target ~ ., data = student.data, subset = train,
family = binomial)
glm.probs <- predict(glm.student, student.data[test, ], type = "response")
glm.pred <- rep("Student", nrow(student.data[test, ]))
attach(student.data)
contrasts(Target)
glm.pred[glm.probs < 0.5] <- "Dropout"
table(glm.pred, student.data$Target[test])
glm.error <- (199 + 75) / (199 + 75 + 516 + 1422)
glm.error
lda.student <- lda(Target ~ ., data = student.data, subset = train)
lda.student
lda.pred <- predict(lda.student, student.data[test, ])
lda.class <- lda.pred$class
table(lda.class, student.data$Target[test])
lda.error <- (216 + 60) / (499 + 60 + 216 + 1437)
lda.error
qda.student <- qda(Target ~ ., data = student.data, subset = train)
qda.student
qda.class <- predict(qda.student, student.data[test, ])$class
table(qda.class, student.data$Target[test])
qda.error <- (207 + 132) / (508 + 132 + 207 + 1365)
qda.error
nb.student <- naiveBayes(Target ~ ., data = student.data, subset = train)
nb.student
nb.class <- predict(nb.student, student.data[test, ])
table(nb.class, student.data$Target[test])
nb.error <- (201 + 159) / (514 + 159 + 201 + 1338)
nb.error
train.X <- as.matrix(student.data[train, sig_predictors])
test.X <- as.matrix(student.data[test, sig_predictors])
train.target <- student.data$Target[train]
n.values <- c(1, 3, 5, 10)
knn.errors <- data.frame(n.value <- n.values,
pred.error <- rep(NA, length(n.values)))
for (x in 1:length(n.values)) {
set.seed(2)
knn.pred <- knn(train.X, test.X, train.target, k = n.values[x])
mean(knn.pred == student.data$Target[test])
}
knn.errors
View(knn.errors)
knn.pred <- knn(train.X, test.X, train.target, k = 1)
View(train.X)
dim(sig_predictors)
length(sig_predictors)
train.X <- as.matrix(student.data[train, sig_predictors[-21]])
test.X <- as.matrix(student.data[test, sig_predictors[-21]])
train.target <- student.data$Target[train]
for (x in 1:length(n.values)) {
set.seed(2)
knn.pred <- knn(train.X, test.X, train.target, k = n.values[x])
mean(knn.pred == student.data$Target[test])
}
View(knn.errors)
knn.errors <- data.frame(n.value = n.values,
pred.error = rep(NA, length(n.values)))
for (x in 1:length(n.values)) {
set.seed(2)
knn.pred <- knn(train.X, test.X, train.target, k = n.values[x])
knn.errors[x, "pred.error"] <- mean(knn.pred == student.data$Target[test])
}
knn.errors
for (x in 1:length(n.values)) {
set.seed(2)
knn.pred <- knn(train.X, test.X, train.target, k = n.values[x])
knn.errors[x, "pred.error"] <- mean(knn.pred != student.data$Target[test])
}
knn.errors
tree.student <- tree(Target ~ ., data = student.data, subset = train)
library(tree)
library(randomForest)
library(gbm)
tree.student <- tree(Target ~ ., data = student.data, subset = train)
summary(tree.student)
plot(tree.student)
text(tree.student, pretty = 0)
tree.pred <- predict(tree.student, student.data[test, ], type = "class")
table(tree.pred, student.data$Target[test])
tree.error <- mean(tree.pred != student.data$Target[test])
tree.error
