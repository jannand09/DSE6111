median(data$medv[index])
}
set.seed(10)
boot.f <- boot(boston.data, med_medv.fn, 1000)
boot.f
# Get tenth percentile of medv
u_10 <- quantile(boston.data$medv, probs = 0.1, na.rm = FALSE)
u_10
ten_medv.fn <- function(data, index) {
quantile(data$medv, probs = 0.1, na.rm = FALSE)
}
set.seed(2)
boot.h <- boot(boston.data, ten_medv.fn, 1000)
boot.h
boot.fn <- function(data, index)
coef(glm(default ~ income + balance, data = data,
subset = index, family = binomial))
boot.fn(default.data, train.default)
set.seed(9)
boot(default.data, boot.fn, 1000)
knitr::opts_chunk$set(echo = TRUE)
library(ISLR2)
library(MASS)
install.packages(leaps)
library(leaps)
install.packages("leaps")
?sample()
# Load Dataset
college.data <- College
# Create vector half the size of college.data that contains random set of indices
set.seed(1)
train <- sample(nrow(college.data), nrow(college.data) / 2)
# Initialize training and test data
college.train <- college.data[train, ]
college.test <- college.datta[-train, ]
# Load Dataset
college.data <- College
# Create vector half the size of college.data that contains random set of indices
set.seed(1)
train <- sample(nrow(college.data), nrow(college.data) / 2)
# Initialize training and test data
college.train <- college.data[train, ]
college.test <- college.data[-train, ]
View(college.data)
?lm()
# Load Dataset
college.data <- College
# Create vector half the size of college.data that contains random set of indices
set.seed(1)
train <- sample(nrow(college.data), 0.8 * nrow(college.data))
# Initialize training and test data
college.train <- college.data[train, ]
college.test <- college.data[-train, ]
?predict()
lm.college <- lm(Apps ~ ., data = college.data, subset = train)
summary(lm.college)
lm.predict <- predict(lm.college, college.test)
lm.college <- lm(Apps ~ ., data = college.data, subset = train)
summary(lm.college)
lm.predict <- predict(lm.college, college.test)
mean((college.test$Apps - lm.predict)^2)
View(college.test)
lm.college <- lm(Apps ~ ., data = college.data, subset = train)
summary(lm.college)
lm.predict <- predict(lm.college, college.test)
lm.mse <- mean((college.test$Apps - lm.predict)^2)
lm.mse
install.packages("glmnet")
library(ISLR2)
library(MASS)
library(leaps)
library(glmnet)
knitr::opts_chunk$set(echo = TRUE)
library(ISLR2)
library(MASS)
library(leaps)
library(glmnet)
# Load Dataset
college.data <- College
college.data <- na.omit(college.data)
# Create vector half the size of college.data that contains random set of indices
set.seed(1)
train <- sample(nrow(college.data), 0.8 * nrow(college.data))
# Initialize training and test data
college.train <- college.data[train, ]
college.test <- college.data[-train, ]
lm.college <- lm(Apps ~ ., data = college.data, subset = train)
summary(lm.college)
lm.predict <- predict(lm.college, college.test)
lm.mse <- mean((college.test$Apps - lm.predict)^2)
lm.mse
View(college.data)
# Create matrix of x, the predictors, and vector of y, the response
x <- model.matrix(Apps ~ ., college.data)[, -2]
y <- college.data$Apps
# Create a lambda grid and use it to form ridge regression model
lambda.grid <- 10^seq(10, -2, length = 100)
ridge.mod <- glmnet(x[train, ], y[train], alpha = 0, lambda = grid, thresh = 1e-12)
# Create matrix of x, the predictors, and vector of y, the response
x <- model.matrix(Apps ~ ., college.data)[, -2]
y <- college.data$Apps
# Create a lambda grid and use it to form ridge regression model
lambda.grid <- 10^seq(10, -2, length = 100)
ridge.mod <- glmnet(x[train, ], y[train], alpha = 0, lambda = lambda.grid, thresh = 1e-12)
# Determine best lambda, or tuning parameter, using cross-validation
set.seed(2)
cv.out <- cv.glmnet(x[train, ], y[train], alpha = 0)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam
# Predict response of test data using ridge regression and calculate MSE
ridge.pred <- predict(ridge.mod, s = bestlam, newx = x[-train, ])
ridge.mse <- mean((ridge.pred - y[-train])^2)
# Create lasso model on the college dataset
lasso.mod <- glmnet(x[train, ], y[train], alpha = 1, lambda = lambda.grid)
plot(lasso.mod)
# Perform cross-validation to determine best lambda or tuning parameter
set.seed(3)
cv.out <- cv.glmnet(x[train, ], y[train], alpha = 1)
plot(cv.out)
bestlam2<- cv.out$lambda.min
# Predict response of test data and calculate MSE
lasso.pred <- predict(lasso.mod, s = bestlam2, newx = x[-train, ])
lasso.mse <- mean((lasso.pred - y[-train])^2)
lasso.mse
install.packages("pls")
library(ISLR2)
library(MASS)
library(leaps)
library(glmnet)
library(pls)
# Create PCR model on training data
set.seed(4)
pcr.fit <- pcr(Apps ~ ., data = college.data, subset = train, scale = T,
validation = "CV")
validationplot(pcr.fit, val.type = "MSEP")
?validationplot
# Create PCR model on training data
set.seed(4)
pcr.fit <- pcr(Apps ~ ., data = college.data, subset = train, scale = T,
validation = "CV")
validationplot(pcr.fit, val.type = "MSEP")
axis(side=1, at=seq(1, 20, by=1))
# Predict the number of applications using the PCR model
pcr.pred <- predict(pcr.fit, x[-train, ], ncomp = 5)
pcr.mse <- mean((pcr.pred - y[-train])^2)
pcr.mse
pcr.fit2 <- pcr(y ~ x, scale = T, ncomp = 5)
View(x)
# Create PLS model on the college data
set.seed(5)
pls.fit <- plsr(Apps ~ ., data = college.data, subset = train, scale = T,
validation = "CV")
summary(pls.fit)
validationplot(pls.fit, val.type = "MSEP")
axis(side=1, at=seq(1, 20, by=1))
# Predict number of applications using PLS
pls.pred <- predict(pls.fit, x[-train, ], ncomp = 7)
pls.mse <- mean((pls.pred - y[-train])^2)
pls.mse
mse.models <- data.frame(
model = c("least.squares", "ridge.regression", "lasso", "pcr", "pls"),
mse = c(lm.mse, ridge.mse, lasso.mse, pcr.mse, pls.mse),
stringsAsFactors = F
)
mse.models
boston.data <- Boston
boston.data <- na.omit(college.data)
# Create vector half the size of college.data that contains random set of indices
set.seed(7)
train <- sample(nrow(boston.data), 0.8 * nrow(boston.data))
test <- (!train)
boston.data <- Boston
boston.data <- na.omit(college.data)
# Create vector half the size of college.data that contains random set of indices
set.seed(7)
train <- sample(nrow(boston.data), 0.8 * nrow(boston.data))
test <- (-train)
boston.data <- Boston
boston.data <- na.omit(college.data)
# Create vector half the size of college.data that contains random set of indices
set.seed(7)
train <- sample(c(TRUE, FALSE), nrow(boston.data), replace = T)
test <- (!train)
View(boston.data)
boston.data <- Boston
boston.data <- na.omit(boston.data)
# Create vector half the size of college.data that contains random set of indices
set.seed(7)
train <- sample(c(TRUE, FALSE), nrow(boston.data), replace = T)
test <- (!train)
View(boston.data)
# Best Subset Selection
regfit.full <- regsubsets(crim ~ ., data = boston.data, nvmax = 13)
summary(regfitfull)
# Best Subset Selection
regfit.full <- regsubsets(crim ~ ., data = boston.data, nvmax = 13)
summary(regfit.full)
# Best Subset Selection
regfit.full <- regsubsets(crim ~ ., data = boston.data, nvmax = 13)
reg.summary <- summary(regfit.full)
par(mfrow = c(2,2))
plot(reg.summary$rss, xlab = "Number of Variables", ylab = "RSS", type = "l")
plot(reg.summary$adjr2, xlab = "Number of Variables", ylab = "Adjusted RSq",
type = "l")
par(mfrow = c(2,2))
plot(reg.summary$rss, xlab = "Number of Variables", ylab = "RSS", type = "l")
plot(reg.summary$adjr2, xlab = "Number of Variables", ylab = "Adjusted RSq",
type = "l")
which.max(reg$summaryadjr2)
par(mfrow = c(2,2))
plot(reg.summary$rss, xlab = "Number of Variables", ylab = "RSS", type = "l")
plot(reg.summary$adjr2, xlab = "Number of Variables", ylab = "Adjusted RSq",
type = "l")
which.max(reg.summary$adjr2)
par(mfrow = c(2,2))
plot(reg.summary$rss, xlab = "Number of Variables", ylab = "RSS", type = "l")
plot(reg.summary$adjr2, xlab = "Number of Variables", ylab = "Adjusted RSq",
type = "l")
which.max(reg.summary$adjr2)
points(9, reg.summary$adjr2[9], col = "red", cex = 2, pch = 20)
coeff(regfit.full, 9)
coef(regfit.full, 9)
# Ridge Regression
x2 <- model.matrix(crim ~ ., boston.data)[, -1]
y2 <- boston.data$crim
ridge.boston <- glmnet(x, y, alpha = 0, lambda = lambda.grid)
summary(ridge.boston)
# Ridge Regression
x2 <- model.matrix(crim ~ ., boston.data)[, -1]
y2 <- boston.data$crim
set.seed(8)
cv.out <- cv.glmnet(x, y, alpha = 0)
bestlam3 <- cv.out$lambda.min
out <- glmnet(x, y, alpha = 0)
predict(out, type = "coefficients", s = bestlam3)[1:13,]
# Ridge Regression
x2 <- model.matrix(crim ~ ., boston.data)[, -1]
y2 <- boston.data$crim
set.seed(8)
cv.out <- cv.glmnet(x2, y2, alpha = 0)
bestlam3 <- cv.out$lambda.min
out <- glmnet(x2, y2, alpha = 0)
predict(out, type = "coefficients", s = bestlam3)[1:13,]
# The Lasso
set.seed(9)
cv.out <- cv.glmnet(x2, y2, alpha = 1)
bestlam4 <- cv.out$lambda.min
out <- glmnet(x2, y2, alpha = 1, lambda = lambda.grid)
lasso.coef <- predict(out, type = "coefficients", s = bestlam4)[1:13,]
lasso.coef
# PCR
set.seed(11)
pcr.boston <- pcr(crim ~ ., data = boston.data, scale = T,
validation = "CV")
validationplot(pcr.boston, val.type = "MSEP")
# Fit PCR to entire data set using M = 8
pcr.boston <- pcr(y2 ~ x2, scale = T, ncomp = 5)
summary(pcr.boston)
# Use Validation-Set Approach to Determine Best Subset Selection Model
regfit.best <- regsubsets(crim ~ ., data = boston.data[train, ], nvmax = 13)
# Create test matrix
test.mat <- model.matrix(crim ~ ., data = boston.data[test, ])
# Compute test MSE for all possible amounts of variables used in the model
val.errors <- rep(NA, 13)
for (i in 1:13) {
coefi <- coef(regfit.best, id = i)
pred <- test.mat[, names(coefi)] %*% coefi
val.errors[i] <- mean((boston.data$crim[test] - pred)^2)
}
# Get coefficient estimates for model with best subset of variables
best.subset <- which.min(val.errors)
coef(regfit.best, best.subset)
# Predict the number of applications using the PCR model
pcr.pred <- predict(pcr.fit, x[-train, ], ncomp = 17)
pcr.mse <- mean((pcr.pred - y[-train])^2)
pcr.mse
mse.models <- data.frame(
model = c("least.squares", "ridge.regression", "lasso", "pcr", "pls"),
mse = c(lm.mse, ridge.mse, lasso.mse, pcr.mse, pls.mse),
stringsAsFactors = F
)
mse.models
# Load libraries
library(ISLR2)
library(MASS)
library(leaps)
library(glmnet)
library(pls)
library(boot)
# Load data set
# Code provided by https://archive.ics.uci.edu/dataset/320/student+performance
d1=read.table("student-mat.csv",sep=";",header=TRUE)
setwd("C:/Users/janna/Documents/Merrimack MSDS/DSE6111/Final Project")
# Load libraries
library(ISLR2)
library(MASS)
library(leaps)
library(glmnet)
library(pls)
library(boot)
# Load data set
# Code provided by https://archive.ics.uci.edu/dataset/320/student+performance
d1=read.table("student-mat.csv",sep=";",header=TRUE)
d2=read.table("student-por.csv",sep=";",header=TRUE)
d3=merge(d1,d2,by=c("school","sex","age","address","famsize","Pstatus","Medu","Fedu","Mjob","Fjob","reason","nursery","internet"))
print(nrow(d3)) # 382 students
# Use student data from d2 which is Portuguese class data
student.data <- d2
# Import liver disorder data
liver <- read.table("bupa.data", sep=",", encoding = "UTF-8")
colnames(liver) <- c("mcv", "alkphos", "sgpt", "sgot", "gammagt", "drinks", "selector")
liver <- as.data.frame(liver)
# Check correlations between predictors and the number of drinks
liver_cor <- cor(liver[,-7], use="complete.obs")
print(liver_cor[6, ])
# Multiple linear regression with all predictors
lm.liver <- lm(drinks ~ ., data = liver[,-7])
summary(lm.liver)
# Import wine quality data
white.quality <- read.csv("winequality-white.csv", sep=";", na.strings = "?", stringsAsFactors = T)
View(white.quality)
red.quality <- read.csv("winequality-red.csv", sep=";", na.strings = "?", stringsAsFactors = T)
View(red.quality)
# Check correlations between predictors and the number of drinks
white_cor <- cor(white.quality, use="complete.obs")
print(white_cor[12, ])
# Multiple linear regression with all predictors
lm.white <- lm(quality ~ ., data = white.quality)
summary(lm.white)
# Create training data
train.white <- sample(nrow(white.quality), 0.8 * nrow(white.quality))
# Train the multiple linear regression model with selected predictors
lm.train <- lm(quality ~ volatile.acidity + residual.sugar + free.sulfur.dioxide
+ density + pH + sulphates + alcohol, data = white.quality,
subset = train.white)
summary(lm.train)
lm.predict <- predict(lm.train, white.quality[-train.white, ])
lm.mse <- mean((lm.predict - white.quality[-train.white, ]$quality)^2)
lm.mse
# Ridge Regression
# Create matrix of x, the predictors, and vector of y, the response
x <- model.matrix(quality ~ ., white.quality)[, -12]
y <- white.quality$quality
# Create a lambda grid and use it to form ridge regression model
lambda.grid <- 10^seq(10, -2, length = 100)
ridge.mod <- glmnet(x[train.white, ], y[train.white], alpha = 0, lambda = lambda.grid,
thresh = 1e-12)
# Determine the bets lambda, or tuning parameter, using cross-validation
set.seed(2)
cv.out <- cv.glmnet(x[train.white, ], y[train.white], alpha = 0)
plot(cv.out)
bestlam.ridge <- cv.out$lambda.min
bestlam.ridge
# Predict the response of test data using ridge regression with best tuning parameter
ridge.pred <- predict(ridge.mod, s = bestlam.ridge, newx = x[-train.white, ])
ridge.mse <- mean((ridge.pred - y[-train.white])^2)
ridge.mse
# The Lasso
lasso.mod <- glmnet(x[train.white, ], y[train.white], alpha = 1, lambda = lambda.grid)
plot(lasso.mod)
# Perform cross-validation to determine best tuning parameter
set.seed(4)
cv.out <- cv.glmnet(x[train.white, ], y[train.white], alpha = 1)
plot(cv.out)
bestlam.lasso <- cv.out$lambda.min
bestlam.lasso
# Predict the response of test data and calculate MSE
lasso.pred <- predict(lasso.mod, s = bestlam.lasso, newx = x[-train.white, ])
lasso.mse <- mean((lasso.pred - y[-train.white])^2)
lasso.mse
# Partial Least Squares
# Create PLS model on the white wine quality data
set.seed(5)
pls.fit <- plsr(quality ~ ., data = white.quality, subset = train.white, scale = T,
validation = "CV")
summary(pls.fit)
# Plot MSEP over the number of components
validationplot(pls.fit, val.type = "MSEP")
axis(side=1, at=seq(1, 20, by=1))
# Predict quality of the wine using PLS
pls.pred <- predict(pls.fit, x[-train.white, ], ncomp=3)
pls.mse <- mean((pls.pred - y[-train.white])^2)
pls.mse
# Plot MSEP over the number of components
validationplot(pls.fit, val.type = "MSEP")
pls.pred <- predict(pls.fit, x[-train.white, ], ncomp=2)
View(pls.fit)
train.white <- sample(1:nrow(white.quality), 0.8 * nrow(white.quality))
test.white <- (-train.white)
set.seed(20)
train.white <- sample(1:nrow(white.quality), 0.8 * nrow(white.quality))
test.white <- (-train.white)
test.white <- (!train.white)
test.white <- (-train.white)
y.test <- y[test.white]
# Predict quality of the wine using PLS
pls.pred <- predict(pls.fit, x[-train.white, ], ncomp=2)
pls.mse <- mean((pls.pred - y.test)^2)
summary(pls.fit)
# Predict quality of the wine using PLS
pls.pred <- predict(pls.fit, x[test.white, ], ncomp=2)
# Create training data
set.seed(20)
train.white <- sample(1:nrow(white.quality), 0.5 * nrow(white.quality))
test.white <- (-train.white)
# Train the multiple linear regression model with selected predictors
lm.train <- lm(quality ~ volatile.acidity + residual.sugar + free.sulfur.dioxide
+ density + pH + sulphates + alcohol, data = white.quality,
subset = train.white)
summary(lm.train)
lm.predict <- predict(lm.train, white.quality[-train.white, ])
lm.mse <- mean((lm.predict - white.quality[-train.white, ]$quality)^2)
lm.mse
# Ridge Regression
# Create matrix of x, the predictors, and vector of y, the response
x <- model.matrix(quality ~ ., white.quality)[, -12]
y <- white.quality$quality
y.test <- y[test.white]
# Create a lambda grid and use it to form ridge regression model
lambda.grid <- 10^seq(10, -2, length = 100)
ridge.mod <- glmnet(x[train.white, ], y[train.white], alpha = 0, lambda = lambda.grid,
thresh = 1e-12)
# Determine the bets lambda, or tuning parameter, using cross-validation
set.seed(2)
cv.out <- cv.glmnet(x[train.white, ], y[train.white], alpha = 0)
plot(cv.out)
bestlam.ridge <- cv.out$lambda.min
bestlam.ridge
# Predict the response of test data using ridge regression with best tuning parameter
ridge.pred <- predict(ridge.mod, s = bestlam.ridge, newx = x[-train.white, ])
ridge.mse <- mean((ridge.pred - y[-train.white])^2)
ridge.mse
# The Lasso
lasso.mod <- glmnet(x[train.white, ], y[train.white], alpha = 1, lambda = lambda.grid)
plot(lasso.mod)
# Perform cross-validation to determine best tuning parameter
set.seed(4)
cv.out <- cv.glmnet(x[train.white, ], y[train.white], alpha = 1)
plot(cv.out)
bestlam.lasso <- cv.out$lambda.min
bestlam.lasso
# Predict the response of test data and calculate MSE
lasso.pred <- predict(lasso.mod, s = bestlam.lasso, newx = x[-train.white, ])
lasso.mse <- mean((lasso.pred - y[-train.white])^2)
lasso.mse
# Partial Least Squares
# Create PLS model on the white wine quality data
set.seed(5)
pls.fit <- plsr(quality ~ ., data = white.quality, subset = train.white, scale = T,
validation = "CV")
summary(pls.fit)
# Plot MSEP over the number of components
validationplot(pls.fit, val.type = "MSEP")
axis(side=1, at=seq(1, 20, by=1))
# Predict quality of the wine using PLS
pls.pred <- predict(pls.fit, x[test.white, ], ncomp=2)
pls.mse <- mean((pls.pred - y.test)^2)
pls.mse
#
pls.fit$coefficients
# Predict quality of the wine using PLS
pls.pred <- predict(pls.fit, x[-1899, ], ncomp=2)
# Predict quality of the wine using PLS
pls.pred <- predict(pls.fit, x[train.white, ], ncomp=2)
# Predict quality of the wine using PLS
pls.pred <- predict(pls.fit, x[test.white, ], ncomp=2)
# Predict quality of the wine using PLS
pls.pred <- predict.mvr(pls.fit, x[test.white, ], ncomp=2)
predplot(pls.fit)
# Predict quality of the wine using PLS
pls.pred <- predict(pls.fit, x[test.white, ], ncomp=2)
# Use validation set approach to determine best subset selection model
regit.best <- regsubsets(quality ~ ., data = white.quality[train, ], nvmax = 11)
# Use validation set approach to determine best subset selection model
regit.best <- regsubsets(quality ~ ., data = white.quality[train.white, ], nvmax = 11)
# Create test matrix
test.mat <- model.matrix(quality ~ ., data = white.quality[test.white, ])
# Compute test MSE for all possible amounts of variables used in the model
val.errors <- rep(NA, 13)
for (i in 1:11) {
coefi <- coef(regfit.best, id = i)
pred <- test.mat[, names(coefi)] %*% coefi
val.errors[i] <- mean((white.quality$quality[test] - pred)^2)
}
# Use validation set approach to determine best subset selection model
regfit.best <- regsubsets(quality ~ ., data = white.quality[train.white, ], nvmax = 11)
# Create test matrix
test.mat <- model.matrix(quality ~ ., data = white.quality[test.white, ])
# Compute test MSE for all possible amounts of variables used in the model
val.errors <- rep(NA, 13)
for (i in 1:11) {
coefi <- coef(regfit.best, id = i)
pred <- test.mat[, names(coefi)] %*% coefi
val.errors[i] <- mean((white.quality$quality[test] - pred)^2)
}
regfit.best <- regsubsets(quality ~ ., data = white.quality[train.white, ], nvmax = 11)
# Create test matrix
test.mat <- model.matrix(quality ~ ., data = white.quality[test.white, ])
# Compute test MSE for all possible amounts of variables used in the model
val.errors <- rep(NA, 13)
for (i in 1:11) {
coefi <- coef(regfit.best, id = i)
pred <- test.mat[, names(coefi)] %*% coefi
val.errors[i] <- mean((white.quality$quality[test.white] - pred)^2)
}
# Get coefficient estimates for model with best subset collection
best.subset <- while.min(val.errors)
best.subset <- which.min(val.errors)
best.subset
coef(regfit.best, best.subset)
val.errors[best.subset]
