---
title: "Annand Final Project Code"
author: "Joseph Annand"
date: "2023-12-15"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Load Libraries


```{r}
library(ISLR2)
library(MASS)
library(leaps)
library(glmnet)
library(pls)
library(boot)
library(tree)
library(randomForest)
library(gbm)
library(BART)
```


## Load wine data sets


```{r}
# Import wine quality data
wine.data <- read.csv("winequality-white.csv", sep=";", na.strings = "?", stringsAsFactors = T)
View(wine.data)
```


The data set consists of 4898 observations with 12 attributes connected to each observation. Each observation is a record of 12 different characteristics of a different type of wine. The characteristics of interest include fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxides, total sulfur dioxides, total sulfur dioxides, density, pH, sulphates, alcohol, and quality.

For the purpose of this research, we are interested in determining the best regression model to predict the quality of the wine using some, or all, of the other characteristics in the data set.


```{r}
white_cor <- cor(wine.data, use="complete.obs")
print(white_cor[12, ])
```


The table shows the correlations between each attribute and quality. The correlation to quality is 1.000 as expected. There are not any other attributes particularly strongly correlated to the quality of white wine. Alcohol and density have the correlation coefficients of greatest magnitude. Chlorides, volatile acidity, and total sulfur dioxide are next strongly correlated to quality.


## Track test errors across models


```{r}
model.errors <- data.frame(model = c("Least Squares", "Ridge Regression", "The LASSO", "PLS",
                                     "Best Subset", "Forward Stepwise", "Backward Stepwise", "Tree", 
                                     "Pruned Tree", "Bagging", "RF", "Boosting"),
                           test.error = rep(NA, 12))
```


### Multiple Linear Regression


```{r}
lm.wine <- lm(quality ~ ., data = wine.data)
summary(lm.wine)
```


The F-statistic for the multiple linear regression with all predictors is well above 1, meaning our regression fit is statistically significant. The predictors that are statistically significant and have a p-value less than 0.05 are residual sugar, free sulfur dioxide, density, pH, sulphates, and alcohol.


```{r}
# Create training and test data
set.seed(10)
train.wine <- sample(1:nrow(wine.data), 0.5 * nrow(wine.data))
test.wine <- (-train.wine)
```


```{r}
# Train a linear model with statistically significant predictors
lm.train <- lm(quality ~ volatile.acidity + residual.sugar + free.sulfur.dioxide
               + density + pH + sulphates + alcohol, data = wine.data,
               subset = train.wine)
summary(lm.train)
```


A multiple linear regression model was trained using half of the observations from the white wine data set. Unlike the first model that included all the possible predictor variables, the trained model used the seven statistically significant predictors from the first model. The F-statistic for the trained model is still much greater than one. All the p-values for the predictors are less than 0.05, so each predictor is significant in this model.


```{r}
# Use trained linear model to predict the wine quality
lm.predict <- predict(lm.train, wine.data[test.wine, ])
lm.mse <- mean((lm.predict - wine.data$quality[test.wine])^2)
lm.mse

model.errors[model.errors$model == "Least Squares", "test.error"] <- lm.mse
```


The test mean squared error calculated using the trained model is approximately 0.570, meaning that on average, the prediction of the quality rating for a white wine is off by 0.755 units. 


```{r}
# Train the first linear model
first.lm.wine <- lm(quality ~ ., data = wine.data, subset = train.wine)
summary(first.lm.wine)

# Use first linear model to predict the wine quality
first.lm.predict <- predict(first.lm.wine, wine.data[test.wine, ])
first.lm.mse <- mean((first.lm.predict - wine.data$quality[test.wine])^2)
first.lm.mse
```


Rounding to three decimal places, the test mean squared error for the trained linear model with all predictors is 0.573, which is slightly greater than that of the trained model with seven predictors. The F-statistic is noticeably lower for the model with all predictors, but it is still well above 1. 


### Ridge Regression


```{r}
# Create matrix of x, the predictors, and vector of y, the response
x <- model.matrix(quality ~ ., wine.data)[, -1]
y <- wine.data$quality

y.test <- y[test.wine]
```



```{r}
# Create a lambda grid and use it to form ridge regression model
lambda.grid <- 10^seq(10, -2, length = 100)
ridge.mod <- glmnet(x[train.wine, ], y[train.wine], alpha = 0, lambda = lambda.grid,
                    thresh = 1e-12)
summary(ridge.mod)
```


```{r}
# Determine the best lambda, or tuning parameter, using cross-validation
set.seed(2)
cv.out <- cv.glmnet(x[train.wine, ], y[train.wine], alpha = 0)
plot(cv.out)
bestlam.ridge <- cv.out$lambda.min
bestlam.ridge
```


Cross-validation is used to determine the best lambda, or tuning parameter for the ridge regression model. The plot shows the mean squared errors plotted over the log of the tuning parameter. The tuning parameter that minimizes the mean squared error is 0.0384732.


```{r}
# Predict the response of test data using ridge regression with best tuning parameter
ridge.pred <- predict(ridge.mod, s = bestlam.ridge, newx = x[test.wine, ])
ridge.mse <- mean((ridge.pred - y.test)^2)
ridge.mse

model.errors[model.errors$model == "Ridge Regression", "test.error"] <- ridge.mse
```


The ridge regression model yields a test mean squared error of 0.574, which is very similar to the test error the linear model with all predictors yielded.


### The Lasso


```{r}
lasso.mod <- glmnet(x[train.wine, ], y[train.wine], alpha = 1, lambda = lambda.grid)
plot(lasso.mod)
```


A Lasso model is trained using the grid of lambda values. A plot of coefficients over L1 show that many coefficients converge to zero or near zero.


```{r}
# Perform cross-validation to determine best tuning parameter
set.seed(2)
cv.out <- cv.glmnet(x[train.wine, ], y[train.wine], alpha = 1)
plot(cv.out)
bestlam.lasso <- cv.out$lambda.min
bestlam.lasso
```


The tuning parameter that minimizes the mean squared error for the Lasso model is 0.0005713149. With a tuning parameter so close to zero, the shrinkage effect on the coefficients is minimal and the model is likely to not change the test error significantly compared to least squares regression.


```{r}
# Predict the response of test data and calculate MSE
lasso.pred <- predict(lasso.mod, s = bestlam.lasso, newx = x[test.wine, ])
lasso.mse <- mean((lasso.pred - y.test)^2)
lasso.mse

model.errors[model.errors$model == "The LASSO", "test.error"] <- lasso.mse
```


The Lasso model is used to predict the quality of white wine using the best tuning parameter and the test data. The mean squared error for the Lasso model is 0.575, which is worse than the multiple linear regression and the ridge regression models.


```{r}
lasso.coef <- predict(lasso.mod, type = "coefficients", s = bestlam.lasso)[1:11, ]
lasso.coef
lasso.coef[lasso.coef != 0]
```


Looking at the coefficients of the Lasso model, the coefficient for fixed acidity is zero, which essentially means that the model removed fixed acidity as a predictor of quality.


### Partial Least Squares Regression


```{r}
# Create PLS model on the wine wine quality data
set.seed(2)
pls.fit <- plsr(quality ~ ., data = wine.data, subset = train.wine, scale = T,
                validation = "CV")
summary(pls.fit)
```


A partial least squares model is trained using the training data set and fit using a cross-validation method. The The root mean squared error determined through cross-validation gradually decreases from 1 component to 8 components. The value of the root mean squared error does not change much with the number of components greater than 8. The summary of the partial least squares fit provides the amount of variance that is explained in the predictors and the response. The highest amount of variance explained in the response is 27.95% while the highest amount of variance in the predictors is 100% when 11 components are used.


```{r}
# Plot MSEP over the number of components
validationplot(pls.fit, val.type = "MSEP")
axis(side=1, at=seq(1, 20, by=1))
```


The figure shows the mean squared error plotted over the number of components used within the partial least squares model. As the number fo components increases from 0 to 3, the mean squared error reduces significantly. After more than 3 components are used in the model, the mean squared error does not change much.


```{r}
# Predict quality of the wine using PLS
pls.pred <- predict(pls.fit, newdata = x[test.wine, ], ncomp=3)
pls.mse <- mean((pls.pred - y.test)^2)
pls.mse

model.errors[model.errors$model == "PLS", "test.error"] <- pls.mse
```

The partial least squares model is used to predict the quality of the white wine with 3 as the number of components. The test mean squared error for PLS


## Best Subset Selection

```{r}
# Function to predict response using best subset selection
predict.regsubsets <- function(object, newdata, id, ...) {
  form <- as.formula(object$call[[2]])
  mat <- model.matrix(form, newdata)
  coefi <- coef(object, id = id)
  xvars <- names(coefi)
  mat[, xvars] %*% coefi
}
```


### Validation Set Approach


```{r}
# Use validation set approach to determine best subset selection model
regfit.best <- regsubsets(quality ~ ., data = wine.data[train.wine, ], nvmax = 11)

# Create test matrix
test.mat <- model.matrix(quality ~ ., data = wine.data[test.wine, ])

# Compute test MSE for all possible amounts of variables used in the model
val.errors <- rep(NA, 13)
for (i in 1:11) {
  coefi <- coef(regfit.best, id = i)
  pred <- test.mat[, names(coefi)] %*% coefi
  val.errors[i] <- mean((wine.data$quality[test.wine] - pred)^2)
}

# Get coefficient estimates for model with best subset collection
best.subset <- which.min(val.errors)
val.errors[best.subset]
coef(regfit.best, best.subset)
```


The best subset selection model is used to select the best set of predictors that minimizes the test mean squared error. A validation set approach is used to find the best subset. This approach finds that the best subset of predictors is fixed acidity, volatile acidity, residual sugar, free sulfur dioxide, density, pH, sulphates, and alcohol; which is a total of 8 predictors from the original 11. The minimmized test MSE is 0.573.


### K-fold Cross-validation approach


```{r}
# Best subset selection using cross-validation method

k <- 10
n <- nrow(wine.data)
set.seed(11)
folds <- sample(rep(1:k, length = n))
cv_sub.errors <- matrix(NA, k, 11,
                      dimnames = list(NULL, paste(1:11)))

for (j in 1:k) {
  cv_sub.fit <- regsubsets(quality ~ .,
                          data = wine.data[folds != j, ],
                          nvmax = 11)
  for (i in 1:11) {
    pred.cv_sub <- predict.regsubsets(cv_sub.fit, wine.data[folds == j, ], id = i)
    cv_sub.errors[j, i] <- mean((wine.data$quality[folds == j] - pred.cv_sub)^2)
  }
}

cv_sub.cv.errors <- apply(cv_sub.errors, 2, mean)
par(mfrow = c(1,1))
plot(cv_sub.cv.errors, type = "b")
```


```{r}
which.min(cv_sub.cv.errors)
cv_sub.mse <- cv_sub.cv.errors[["8"]]
cv_sub.mse

model.errors[model.errors$model == "Best Subset", "test.error"] <- cv_sub.mse
```


Best subset selection is performed again using a k-fold cross-validation approach this time. With k = 10, the cross-validation also determines that the best subset contains 8 predictors. The test MSE is calculated to be 0.568.

## Forward Stepwise Selection


```{r}
k <- 10
n <- nrow(wine.data)
set.seed(11)
folds <- sample(rep(1:k, length = n))
f.cv.errors <- matrix(NA, k, 11,
                    dimnames = list(NULL, paste(1:11)))

for (j in 1:k) {
  fstep.fit <- regsubsets(quality ~ .,
                         data = wine.data[folds != j, ],
                         nvmax = 11,
                         method = "forward")
  for (i in 1:11) {
    pred.forward <- predict.regsubsets(fstep.fit, wine.data[folds == j, ], id = i)
    f.cv.errors[j, i] <- mean((wine.data$quality[folds == j] - pred.forward)^2)
  }
}

forward.cv.errors <- apply(f.cv.errors, 2, mean)
forward.cv.errors
par(mfrow = c(1,1))
plot(forward.cv.errors, type = "b")
```


```{r}
which.min(forward.cv.errors)
forward.mse <- forward.cv.errors[["8"]]
forward.mse

model.errors[model.errors$model == "Forward Stepwise", "test.error"] <- forward.mse
```


## Backward Stepwise Selection


```{r}
# Backward Step-wise Subset Selection Using Cross Validation

k <- 10
n <- nrow(wine.data)
set.seed(11)
folds <- sample(rep(1:k, length = n))
b.cv.errors <- matrix(NA, k, 11,
                    dimnames = list(NULL, paste(1:11)))

for (j in 1:k) {
  bstep.fit <- regsubsets(quality ~ .,
                          data = wine.data[folds != j, ],
                          nvmax = 11,
                          method = "backward")
  for (i in 1:11) {
    pred.backward <- predict.regsubsets(fstep.fit, wine.data[folds == j, ], id = i)
    b.cv.errors[j, i] <- 
      mean((wine.data$quality[folds == j] - pred.backward)^2)
  }
}

backward.cv.errors <- apply(b.cv.errors, 2, mean)
backward.cv.errors
par(mfrow = c(1,1))
plot(backward.cv.errors, type = "b")
```


```{r}
which.min(backward.cv.errors)
backward.mse <- backward.cv.errors[["11"]]
backward.mse

model.errors[model.errors$model == "Backward Stepwise", "test.error"] <- backward.mse
```


Both forward and backward step-wise subset selection using cross-validation determine that the best model uses 8 predictors. While forward step-wise yields the same test error as best subset selection, the backward step-wise selection model yields a slightly lower test MSE of 0.563.


## Regression Tree

```{r}
# Create regression tree model for quality as response
tree.wine <- tree(quality ~ ., data = wine.data, subset = train.wine)
summary(tree.wine)
```


```{r}
# Plot the regression tree
plot(tree.wine)
text(tree.wine, pretty = 0)
```


The regression tree uses four predictors to construct the tree: alcohol, volatile acidity, free sulfur dioxide, and chlorides. The tree contains 6 terminal nodes.


```{r}
tree.pred <- predict(tree.wine, newdata = wine.data[test.wine, ])
tree.mse <- mean((tree.pred - y.test)^2)
tree.mse

model.errors[model.errors$model == "Tree", "test.error"] <- tree.mse
```
Using the regression tree to predict the quality of white wine, the test MSE is 0.593.

## Pruned Tree


```{r}
# Use cross-validation to determine best tree size for pruning
tree.cv <- cv.tree(tree.wine)
plot(tree.cv$size, tree.cv$dev, type = "b")
```


```{r}
# Prune tree and plot the tree
prune.wine <- prune.tree(tree.wine, best = 6)
plot(prune.wine)
text(prune.wine, pretty = 0)
```


```{r}
# Predict response with pruned tree
prune.pred <- predict(prune.wine, newdata = wine.data[test.wine, ])
prune.mse <- mean((prune.pred - y.test)^2)
prune.mse

model.errors[model.errors$model == "Pruned Tree", "test.error"] <- prune.mse
```

Using cross-validation to determine the ideal size of the tree does not give any new information: the ideal size of a regression tree for our training data is the same as that of the original tree we created. Predictably, the pruned tree with a size of 6 is gives the same tree and yields virtually the same test MSE.

## Bagging


```{r}
set.seed(90)
bag.wine <- randomForest(quality ~ ., data = wine.data, subset = train.wine,
                         mtry = (ncol(wine.data) - 1), importance = T)
bag.wine
```


A bagging model is created to attempt to use a series of bootstrapped regression trees to better predict the quality of white wine. The bagging model considers all predictors as split candidates at each split in the trees. 500 trees are used in the model, the mean of squared residuals is 0.433, and 43.96% of the variance is explained with this model.


```{r}
bag.pred <- predict(bag.wine, newdata = wine.data[test.wine, ])
bag.mse <- mean((bag.pred - y.test)^2)
bag.mse

model.errors[model.errors$model == "Bagging", "test.error"] <- bag.mse
```

Using the bagging model to predict quality in the test data set yields a test MSE of 0.408, which is the lowest of any model thus far.

## Random Forest


```{r}
set.seed(90)
rf.wine <- randomForest(quality ~ ., data = wine.data[train.wine, ],
                        importance = T)
rf.wine
```


Similar to bagging, the random forest tries to predict the response using 500 regression trees. The random forest model, however, uses considers only a random selection of 3 predictors at each split. The model has a slightly lower mean of squared residuals than the bagging model at 0.423, but explains 45.23% of the variance of the model.


```{r}
importance(rf.wine)
varImpPlot(rf.wine)
```


Looking at the graphs indicating the importance of predictors in the random forest model, alcohol, volatile acidity, free sulfur dioxide, and chlorides are revealed to be four of the most important predictors to predict the quality of white wine. These are the same predictors featured in the pruned regression tree model.


```{r}
rf.pred <- predict(rf.wine, newdata = wine.data[test.wine, ])
rf.mse <- mean((rf.pred - y.test)^2)
rf.mse

model.errors[model.errors$model == "RF", "test.error"] <- rf.mse
```


The random forest model yields the lowest test MSE thus far at 0.407 when applied to the test data set.

## Boosting


```{r}
tunings <- c(0.001, 0.005, 0.01, 0.02, 0.03)
boost.errors <- data.frame(shrinkage = tunings,
                           training.error = rep(NA, length(tunings)),
                           test.error = rep(NA, length(tunings)))

for (x in 1:length(tunings)) {
  set.seed(91)
  boost.wine <- gbm(quality ~ ., data = wine.data[train.wine, ],
                    distribution = "gaussian", n.trees = 1000,
                    interaction.depth = 4, shrinkage = tunings[x])
  boost.errors[x, "training.error"] <- mean(boost.wine$train.error)
}

for (x in 1:length(tunings)) {
  set.seed(91)
  boost.wine <- gbm(quality ~ ., data = wine.data[train.wine, ],
                    distribution = "gaussian", n.trees = 1000,
                    interaction.depth = 4, shrinkage = tunings[x])
  yhat.boost <- predict(boost.wine, newdata = wine.data[test.wine, ],
                        n.trees = 1000)
  boost.errors[x, "test.error"] <- mean((yhat.boost - y.test)^2)
}

boost.errors
```


A boosting model, which attempts to improve on bagging by sequentially creating trees and using information from previous trees, was created using a set of different shrinkage parameter. Each model used 1000 trees and a tree size equal to 4. As the shrinkage parameter increases, the training error decreases; however, the minimum test error, 0.467, is seen when lambda equals 0.02.


```{r}
model.errors[model.errors$model == "Boosting", "test.error"] <- min(boost.errors$test.error)
```


```{r}
model.errors
```


```{r}
sqrt(min(model.errors$test.error))
```


Least squares regression and the generalized linear models (GLMs) all yield a similar test mean squared error when trying to predict the quality of white wine using the test data set. The regression tree model performs worse on the test data than the GLMs; however, other tree based methods yield significantly lower mean squared errors than all other models. The best of these is the random forest model with a test MSE of 0.407, which means, on average, the random forest will predict a quality rating 0.638 units off the actual value.