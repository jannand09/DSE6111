---
title: "Annand Final Project Code"
author: "Joseph Annand"
date: "2023-12-15"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load Libraries


```{r}
library(ISLR2)
library(MASS)
library(leaps)
library(glmnet)
library(pls)
library(boot)
library(tree)
library(randomForest)
library(gbm)
library(BART)
```


## Load wine data sets


```{r}
# Import wine quality data
wine.data <- read.csv("winequality-white.csv", sep=";", na.strings = "?", stringsAsFactors = T)
View(wine.data)
```


The data set consists of 4898 observations with 12 attributes connected to each observation. Each observation is a record of 12 different characteristics of a different type of wine. The characteristics of interest include fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxides, total sulfur dioxides, total sulfur dioxides, density, pH, sulphates, alcohol, and quality.

For the purpose of this research, we are interested in determining the best regression model to predict the quality of the wine using some, or all, of the other characteristics in the data set.


```{r}
white_cor <- cor(wine.data, use="complete.obs")
print(white_cor[12, ])
```


The table shows the correlations between each attribute and quality. The correlation to quality is 1.000 as expected. There are not any other attributes particularly strongly correlated to the quality of white wine. Alcohol and density have the correlation coefficients of greatest magnitude. Chlorides, volatile acidity, and total sulfur dioxide are next strongly correlated to quality.


### Multiple Linear Regression


```{r}
lm.wine <- lm(quality ~ ., data = wine.data)
summary(lm.wine)
```


The F-statistic for the multiple linear regression with all predictors is well above 1, meaning our regression fit is statistically significant. The predictors that are statistically significant and have a p-value less than 0.05 are residual sugar, free sulfur dioxide, density, pH, sulphates, and alcohol.


```{r}
# Create training and test data
set.seed(10)
train.wine <- sample(1:nrow(wine.data), 0.5 * nrow(wine.data))
test.wine <- (-train.wine)
```


```{r}
# Train a linear model with statistically significant predictors
lm.train <- lm(quality ~ volatile.acidity + residual.sugar + free.sulfur.dioxide
               + density + pH + sulphates + alcohol, data = wine.data,
               subset = train.wine)
summary(lm.train)
```


A multiple linear regression model was trained using half of the observations from the white wine data set. Unlike the first model that included all the possible predictor variables, the trained model used the seven statistically significant predictors from the first model. The F-statistic for the trained model is still much greater than one. All the p-values for the predictors are less than 0.05, so each predictor is significant in this model.


```{r}
# Use trained linear model to predict the wine quality
lm.predict <- predict(lm.train, wine.data[test.wine, ])
lm.mse <- mean((lm.predict - wine.data$quality[test.wine])^2)
lm.mse
```


The test mean squared error calculated using the trained model is approximately 0.570, meaning that on average, the prediction of the quality rating for a white wine is off by 0.755 units. 


```{r}
# Train the first linear model
first.lm.wine <- lm(quality ~ ., data = wine.data, subset = train.wine)
summary(first.lm.wine)

# Use first linear model to predict the wine quality
first.lm.predict <- predict(first.lm.wine, wine.data[test.wine, ])
first.lm.mse <- mean((first.lm.predict - wine.data$quality[test.wine])^2)
first.lm.mse
```


Rounding to three decimal places, the test mean squared error for the trained linear model with all predictors is 0.573, which is slightly greater than that of the trained model with seven predictors. The F-statistic is noticeably lower for the model with all predictors, but it is still well above 1. 


### Ridge Regression


```{r}
# Create matrix of x, the predictors, and vector of y, the response
x <- model.matrix(quality ~ ., wine.data)[, -1]
y <- wine.data$quality

y.test <- y[test.wine]
```



```{r}
# Create a lambda grid and use it to form ridge regression model
lambda.grid <- 10^seq(10, -2, length = 100)
ridge.mod <- glmnet(x[train.wine, ], y[train.wine], alpha = 0, lambda = lambda.grid,
                    thresh = 1e-12)
summaryridge.mod
```


```{r}
# Determine the best lambda, or tuning parameter, using cross-validation
set.seed(2)
cv.out <- cv.glmnet(x[train.wine, ], y[train.wine], alpha = 0)
plot(cv.out)
bestlam.ridge <- cv.out$lambda.min
bestlam.ridge
```


Cross-validation is used to determine the best lambda, or tuning parameter for the ridge regression model. The plot shows the mean squared errors plotted over the log of the tuning parameter. The tuning parameter that minimizes the mean squared error is 0.0384732.


```{r}
# Predict the response of test data using ridge regression with best tuning parameter
ridge.pred <- predict(ridge.mod, s = bestlam.ridge, newx = x[test.wine, ])
ridge.mse <- mean((ridge.pred - y.test)^2)
ridge.mse
```


The ridge regression model yields a test mean squared error of 0.574, which is very similar to the test error the linear model with all predictors yielded.


### The Lasso


```{r}
lasso.mod <- glmnet(x[train.wine, ], y[train.wine], alpha = 1, lambda = lambda.grid)
plot(lasso.mod)
```


A Lasso model is trained using the grid of lambda values. A plot of coefficients over L1 show that many coefficients converge to zero or near zero.


```{r}
# Perform cross-validation to determine best tuning parameter
set.seed(2)
cv.out <- cv.glmnet(x[train.wine, ], y[train.wine], alpha = 1)
plot(cv.out)
bestlam.lasso <- cv.out$lambda.min
bestlam.lasso
```


The tuning parameter that minimizes the mean squared error for the Lasso model is 0.0005713149. With a tuning parameter so close to zero, the shrinkage effect on the coefficients is minimal and the model is likely to not change the test error significantly compared to least squares regression.


```{r}
# Predict the response of test data and calculate MSE
lasso.pred <- predict(lasso.mod, s = bestlam.lasso, newx = x[test.wine, ])
lasso.mse <- mean((lasso.pred - y.test)^2)
lasso.mse
```


The Lasso model is used to predict the quality of white wine using the best tuning parameter and the test data. The mean squared error for the Lasso model is 0.575, which is worse than the multiple linear regression and the ridge regression models.


```{r}
lasso.coef <- predict(lasso.mod, type = "coefficients", s = bestlam.lasso)[1:11, ]
lasso.coef
lasso.coef[lasso.coef != 0]
```


Looking at the coefficients of the Lasso model, the coefficient for fixed acidity is zero, which essentially means that the model removed fixed acidity as a predictor of quality.


### Partial Least Squares Regression


```{r}
# Create PLS model on the wine wine quality data
set.seed(2)
pls.fit <- plsr(quality ~ ., data = wine.data, subset = train.wine, scale = T,
                validation = "CV")
summary(pls.fit)
```


A partial least squares model is trained using the training data set and fit using a cross-validation method. The The root mean squared error determined through cross-validation gradually decreases from 1 component to 8 components. The value of the root mean squared error does not change much with the number of components greater than 8. The summary of the partial least squares fit provides the amount of variance that is explained in the predictors and the response. The highest amount of variance explained in the response is 27.95% while the highest amount of variance in the predictors is 100% when 11 components are used.


```{r}
# Plot MSEP over the number of components
validationplot(pls.fit, val.type = "MSEP")
axis(side=1, at=seq(1, 20, by=1))
```


The figure shows the mean squared error plotted over the number of components used within the partial least squares model. As the number fo components increases from 0 to 3, the mean squared error reduces significantly. After more than 3 components are used in the model, the mean squared error does not change much.


```{r}
# Predict quality of the wine using PLS
pls.pred <- predict(pls.fit, newdata = x[test.wine, ], ncomp=3)
pls.mse <- mean((pls.pred - y.test)^2)
pls.mse
```

The partial least squares model is used to predict the quality of the white wine with 3 as the number of components. The test mean squared error for PLS


## Best Subset Selection

```{r}
# Function to predict response using best subset selection
predict.regsubsets <- function(object, newdata, id, ...) {
  form <- as.formula(object$call[[2]])
  mat <- model.matrix(form, newdata)
  coefi <- coef(object, id = id)
  xvars <- names(coefi)
  mat[, xvars] %*% coefi
}
```


### Validation Set Approach


```{r}
# Use validation set approach to determine best subset selection model
regfit.best <- regsubsets(quality ~ ., data = wine.data[train.wine, ], nvmax = 11)

# Create test matrix
test.mat <- model.matrix(quality ~ ., data = wine.data[test.wine, ])

# Compute test MSE for all possible amounts of variables used in the model
val.errors <- rep(NA, 13)
for (i in 1:11) {
  coefi <- coef(regfit.best, id = i)
  pred <- test.mat[, names(coefi)] %*% coefi
  val.errors[i] <- mean((wine.data$quality[test.wine] - pred)^2)
}

# Get coefficient estimates for model with best subset collection
best.subset <- which.min(val.errors)
val.errors[best.subset]
coef(regfit.best, best.subset)
```


### K-fold Cross-validation approach


```{r}
# Best subset selection using cross-validation method

k <- 10
n <- nrow(wine.data)
set.seed(11)
folds <- sample(rep(1:k, length = n))
cv_sub.errors <- matrix(NA, k, 11,
                      dimnames = list(NULL, paste(1:11)))

for (j in 1:k) {
  cv_sub.fit <- regsubsets(quality ~ .,
                          data = wine.data[folds != j, ],
                          nvmax = 11)
  for (i in 1:11) {
    pred.cv_sub <- predict.regsubsets(cv_sub.fit, wine.data[folds == j, ], id = i)
    cv_sub.errors[j, i] <- mean((wine.data$quality[folds == j] - pred.cv_sub)^2)
  }
}

cv_sub.cv.errors <- apply(cv_sub.errors, 2, mean)
par(mfrow = c(1,1))
plot(cv_sub.cv.errors, type = "b")
```





```{r}
which.min(cv_sub.cv.errors)
cv_sub.mse <- cv_sub.cv.errors[["8"]]
cv_sub.mse
```


## Forward Stepwise Selection


```{r}
k <- 10
n <- nrow(wine.data)
set.seed(11)
folds <- sample(rep(1:k, length = n))
f.cv.errors <- matrix(NA, k, 11,
                    dimnames = list(NULL, paste(1:11)))

for (j in 1:k) {
  fstep.fit <- regsubsets(quality ~ .,
                         data = wine.data[folds != j, ],
                         nvmax = 11,
                         method = "forward")
  for (i in 1:11) {
    pred.forward <- predict.regsubsets(fstep.fit, wine.data[folds == j, ], id = i)
    f.cv.errors[j, i] <- mean((wine.data$quality[folds == j] - pred.forward)^2)
  }
}

forward.cv.errors <- apply(f.cv.errors, 2, mean)
forward.cv.errors
par(mfrow = c(1,1))
plot(forward.cv.errors, type = "b")
```


```{r}
which.min(forward.cv.errors)
forward.mse <- forward.cv.errors[["8"]]
forward.mse
```


## Backward Stepwise Selection


```{r}
# Backward Step-wise Subset Selection Using Cross Validation

k <- 10
n <- nrow(wine.data)
set.seed(11)
folds <- sample(rep(1:k, length = n))
b.cv.errors <- matrix(NA, k, 11,
                    dimnames = list(NULL, paste(1:11)))

for (j in 1:k) {
  bstep.fit <- regsubsets(quality ~ .,
                          data = wine.data[folds != j, ],
                          nvmax = 11,
                          method = "backward")
  for (i in 1:11) {
    pred.backward <- predict.regsubsets(fstep.fit, wine.data[folds == j, ], id = i)
    b.cv.errors[j, i] <- 
      mean((wine.data$quality[folds == j] - pred.backward)^2)
  }
}

backward.cv.errors <- apply(b.cv.errors, 2, mean)
backward.cv.errors
par(mfrow = c(1,1))
plot(backward.cv.errors, type = "b")
```


```{r}
which.min(backward.cv.errors)
backward.mse <- backward.cv.errors[["11"]]
backward.mse
```


## Regression Tree

```{r}
# Create regression tree model for quality as response
tree.wine <- tree(quality ~ ., data = wine.data, subset = train.wine)
summary(tree.wine)
```


```{r}
# Plot the regression tree
plot(tree.wine)
text(tree.wine, pretty = 0)
```


```{r}
tree.pred <- predict(tree.wine, newdate = wine.data[test.wine, ])
tree.mse <- mean((tree.pred - y.test)^2)
tree.mse
```


## Pruned Tree


```{r}
# Use cross-validation to determine best tree size for pruning
tree.cv <- cv.tree(tree.wine)
plot(tree.cv$size, tree.cv$dev, type = "b")
```


```{r}
# Prune tree and plot the tree
prune.wine <- prune.tree(tree.wine, best = 6)
plot(prune.wine)
text(prune.wine, pretty = 0)
```


```{r}
# Predict response with pruned tree
prune.pred <- predict(prune.wine, newdata = wine.data[test.wine, ])
prune.mse <- mean((prune.pred - y.test)^2)
prune.mse
```


## Bagging


```{r}
set.seed(90)
bag.wine <- randomForest(quality ~ ., data = wine.data, subset = train.wine,
                         mtry = (ncol(wine.data) - 1), importance = T)
bag.wine
```


```{r}
bag.pred <- predict(bag.wine, newdata = wine.data[test.wine, ])
bag.mse <- mean((bag.pred - y.test)^2)
bag.mse
```


## Random Forest


```{r}
set.seed(90)
rf.wine <- randomForest(quality ~ ., data = wine.data[train.wine, ])
rf.wine
```


```{r}
varImpPlot(rf.wine)
```



```{r}
rf.pred <- predict(rf.wine, newdata = wine.data[test.wine, ])
rf.mse <- mean((rf.pred - y.test)^2)
rf.mse
```


## Boosting


```{r}
tunings <- c(0.001, 0.005, 0.01, 0.015, 0.020)
boost.errors <- data.frame(lambda = tunings,
                           training.error = rep(NA, length(tunings)),
                           test.error = rep(NA, length(tunings)))

for (x in 1:length(tunings)) {
  set.seed(91)
  boost.wine <- gbm(quality ~ ., data = wine.data[train.wine, ],
                    distribution = "gaussian", n.trees = 1000,
                    interaction.depth = 4, shrinkage = tunings[x])
  boost.errors[x, "training.error"] <- mean(boost.wine$train.error)
}

for (x in 1:length(tunings)) {
  set.seed(91)
  boost.wine <- gbm(quality ~ ., data = wine.data[train.wine, ],
                    distribution = "gaussian", n.trees = 1000,
                    interaction.depth = 4, shrinkage = tunings[x])
  yhat.boost <- predict(boost.wine, newdata = wine.data[test.wine, ],
                        n.trees = 1000)
  boost.errors[x, "test.error"] <- mean((yhat.boost - y.test)^2)
}

boost.errors
```

