---
title: "Annand Module 07 Lab 01"
author: "Joseph Annand"
date: "2023-12-10"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(ISLR2)
library(MASS)
library(tree)
```


## Question 8


### Part A

```{r}
# Split data into training and test data
set.seed(1)
train <- sample(1:nrow(Carseats), nrow(Carseats) / 2)
sales.test <- Carseats[-train, "Sales"]
```


### Part B


```{r}
tree.sales <- tree(Sales ~ ., data = Carseats, subset = train)
summary(tree.sales)

plot(tree.sales)
text(tree.sales, pretty = 0)
```


```{r}
sales.pred <- predict(tree.sales, newdata = Carseats[-train, ])
plot(sales.pred, sales.test)
abline(0, 1)
mean((sales.pred - sales.test)^2)
```


### Part C


```{r}
cv.sales <- cv.tree(tree.sales)
plot(cv.sales$size, cv.sales$dev, type = "b")
```


```{r}
prune.sales <- prune.tree(tree.sales, best = 14)
plot(prune.sales)
text(prune.sales, pretty = 0)
```


```{r}
prune.pred <- predict(prune.sales, newdata = Carseats[-train, ])
plot(prune.pred, sales.test)
mean((prune.pred - sales.test)^2)
```


Pruning the data set resulted in a higher MSE than when it was unpruned.


### Part D


```{r}
library(randomForest)
set.seed(2)
bag.sales <- randomForest(Sales ~ ., data = Carseats, subset = train,
                          mtry = (ncol(Carseats) - 1), importance = T)
bag.sales
```


```{r}
bag.pred <- predict(bag.sales, newdata = Carseats[-train, ])
plot(bag.pred, sales.test)
abline(0, 1)
mean((bag.pred - sales.test)^2)
```


### Part E


```{r}
set.seed(2)
# Create random forest model using default m = p/3 
rf.sales <- randomForest(Sales ~ ., data = Carseats, subset = train,
                         importance = T)
rf.pred <- predict(rf.sales, newdata = Carseats[-train, ])
mean((rf.pred - sales.test)^2)
```


```{r}
set.seed(2)
# Create random forest model using default m = 6 
rf.sales6 <- randomForest(Sales ~ ., data = Carseats, subset = train,
                         mtry = 6, importance = T)
rf.pred6 <- predict(rf.sales6, newdata = Carseats[-train, ])
mean((rf.pred6 - sales.test)^2)
```


```{r}
set.seed(2)
# Create random forest model using default m = 9 
rf.sales9 <- randomForest(Sales ~ ., data = Carseats, subset = train,
                         mtry = 9, importance = T)
rf.pred9 <- predict(rf.sales9, newdata = Carseats[-train, ])
mean((rf.pred9 - sales.test)^2)
```


Setting m, the number of variables considered at each split, equal to 6 yields the lowest test MSE.


```{r}
importance(rf.sales6)
varImpPlot(rf.sales6)
```


Across all the trees considered, Price and ShelveLoc are the two most important variables.


### Part F


```{r}
# Analyze data using BART
library(BART)
x <- Carseats[, 2:11]
y <- Carseats[, "Sales"]
xtrain <- x[train, ]
ytrain <- y[train]
xtest <- x[-train, ]
ytest <- y[-train]

# Run BART with default settings
set.seed(1)
bartfit <- gbart(xtrain, ytrain, x.test = xtest)

bart.pred <- bartfit$yhat.test.mean
mean((ytest - bart.pred)^2)
```


BART yields a significantly lower test error than bagging and random forests.


## Question 9


### Part A


```{r}
# Divide OJ data set into training and test data
set.seed(3)
train.oj <- sample(1:nrow(OJ), 800)
test.oj <- OJ[-train.oj, ]
```


### Part B


```{r}
# Fit a classification tree to OJ data with Purchase as the response
tree.oj <- tree(Purchase ~ ., data = OJ, subset = train.oj)
summary(tree.oj)
```


The variables used in the classification tree construction are LoyalCH, PriceDiff, PriceMM, and SalePriceMM. There are 9 terminal nodes, and the training error rate is 18.12%.


### Part C


```{r}
tree.oj
```


Taking a look at node 24, the classification process can be described as followed: if LoyalCH is greater than 0.5036 and less than 0.764572, and if PriceDiff is less than 0.265, and if SalePriceMM is less than 2.155, then the response is classified as CH.


### Part D


```{r}
plot(tree.oj)
text(tree.oj, pretty = 0)
```


The decision tree shows that generally when customer brand loyalty is the sole predictor in the first two levels of the tree. Price difference is the next major predictor to be used in the tree. Price and sale price of Minute Maid are used in the last level of the tree.


### Part E


```{r}
# Use classification tree to predict responses of test data
oj.pred <- predict(tree.oj, test.oj, type = "class")
# Generate confusion matrix
table(oj.pred, test.oj$Purchase)
```


```{r}
(15 + 31) / (148 + 31 + 15 + 76)
```


The test error rate is 17%.


### Part F


```{r}
# Use cross-validation to determine if pruning the tree may result in better prediction error
set.seed(4)
cv.oj <- cv.tree(tree.oj, FUN = prune.misclass)
names(cv.oj)

cv.oj
```


### Part G


```{r}
# Create plot of cross-validated training error over tree size
plot(cv.oj$size, cv.oj$dev, type = "b")
```


### Part H

Tree size 5 corresponds to the lowest cross-validated classification error.


### Part I


```{r}
prune.oj <- prune.misclass(tree.oj, best = 5)
plot(prune.oj)
text(prune.oj, pretty = 0)

summary(prune.oj)
```


### Part J


The training errors are the same for the pruned tree and the orginal classification tree we created.

### Part K


```{r}
# Predict response with pruned tree
prune.oj.pred <- predict(prune.oj, test.oj, type = "class")
# Generate confusion matrix for pruned tree
table(prune.oj.pred, test.oj$Purchase)
```


```{r}
(15 + 31) / (148 + 31 + 15 + 76)
```


The pruned tree yields the same test error as the original tree.


## Question 10


### Part A


```{r}
hitters.data <- na.omit(Hitters)
hitters.data$Salary <- log(hitters.data$Salary)
```


### Part B


```{r}
hitters.train <- c(1:200)
hitters.test <- c(201:nrow(hitters.data))
```


### Part C


```{r}
library(gbm)
tunings <- c(0.001, 0.01, 0.2, 0.5, 0.75, 1.0)
gbm.training <- data.frame(lambda = tunings,
                           training.error = rep(NA, length(tunings)))
for (x in 1:length(tunings)) {
  set.seed(5)
  boost.hitters <- gbm(Salary ~ ., data = hitters.data[hitters.train, ],
                     distribution = "gaussian", n.trees = 1000,
                     interaction.depth = 4, shrinkage = tunings[x])
  gbm.training[x, "training.error"] <-  mean(boost.hitters$train.error)
}

gbm.training
```


```{r}
plot(gbm.training$lambda, gbm.training$training.error, type = "b")
```


### Part D


```{r}
gbm.test <- data.frame(lambda = tunings,
                       test.error = rep(NA, length(tunings)))
for (x in 1:length(tunings)) {
  set.seed(5)
  boost.hitters <- gbm(Salary ~ ., data = hitters.data[hitters.train, ],
                     distribution = "gaussian", n.trees = 1000,
                     interaction.depth = 4, shrinkage = tunings[x])
  yhat.boost <- predict(boost.hitters, newdata = hitters.data[hitters.test, ],
                        n.trees = 1000)
  gbm.test[x, "test.error"] <-  mean((yhat.boost - hitters.data$Salary[hitters.test])^2)
}

gbm.test

plot(gbm.test$lambda, gbm.test$test.error, type = "b")
```


### Part E


```{r}
# Create LS regression with Salary as the predictor and determine test MSE
lm.hitters <- lm(Salary ~ ., data = hitters.data, subset = hitters.train)
lm.predict <- predict(lm.hitters, newdata = hitters.data[hitters.test, ])
lm.mse <- mean((lm.predict - hitters.data$Salary[hitters.test])^2)
lm.mse
```


```{r}
library(glmnet)
x <- model.matrix(Salary ~ ., hitters.data)[, -1]
y <- hitters.data$Salary

lambda.grid <- 10^seq(10, -2, length = 100)
lasso.mod <- glmnet(x[hitters.train, ], y[hitters.train], alpha = 1,
                    lambda = lambda.grid)
set.seed(7)
cv.out <- cv.glmnet(x[hitters.train, ], y[hitters.train], alpha = 1)
bestlam <- cv.out$lambda.min

lasso.pred <- predict(lasso.mod, s = bestlam, newx = x[hitters.test, ])
lasso.mse <- mean((lasso.pred - y[hitters.test])^2)
lasso.mse
```


The boosting MSE when lambda = 0.01 is much smaller than the MSE of a multiple linear regression and the Lasso.


### Part F


```{r}
# Create boosting model using shrinkage, or tuning parameter, equal to 0.01
set.seed(5)
boost.hitters <- gbm(Salary ~ ., data = hitters.data[hitters.train, ],
                   distribution = "gaussian", n.trees = 1000,
                   interaction.depth = 4, shrinkage = 0.01)
summary(boost.hitters)
```


CAtBat is clearly the most important predictor in the boosting model. CWalks, CRuns, and CRBI are the next highest in importance.


### Part G


```{r}
set.seed(5)
bag.hitters <- randomForest(Salary ~ ., data = hitters.data,
                            subset = hitters.train, mtry = 19,
                            importance = T)
yhat.bag <- predict(bag.hitters, newdata = hitters.data[hitters.test, ])
mean((yhat.bag - hitters.data$Salary[hitters.test])^2)
```


Bagging yields a test MSE of 0.234. This is slightly lower than the test MSE of boosting with tuning parameter equal to 0.01, which was 0.270.
