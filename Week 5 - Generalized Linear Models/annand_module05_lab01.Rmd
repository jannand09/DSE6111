---
title: "Annand Module 05 Lab 01"
author: "Joseph Annand"
date: "2023-11-25"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load Libraries

```{r}
library(ISLR2)
library(MASS)
library(leaps)
library(glmnet)
library(pls)
```


## Question 9


### Part A - Prepare the Data


```{r}
# Load Dataset
college.data <- College
college.data <- na.omit(college.data)

# Create vector half the size of college.data that contains random set of indices
set.seed(1)
train <- sample(nrow(college.data), 0.8 * nrow(college.data))

# Initialize training and test data
college.train <- college.data[train, ]
college.test <- college.data[-train, ]
```


### Part B - Least Squares Regression

```{r}
lm.college <- lm(Apps ~ ., data = college.data, subset = train)

summary(lm.college)

lm.predict <- predict(lm.college, college.test)

lm.mse <- mean((lm.predict - college.test$Apps)^2)
lm.mse
```


### Part C - Ridge Regression

```{r}
# Create matrix of x, the predictors, and vector of y, the response
x <- model.matrix(Apps ~ ., college.data)[, -2]
y <- college.data$Apps

# Create a lambda grid and use it to form ridge regression model
lambda.grid <- 10^seq(10, -2, length = 100)
ridge.mod <- glmnet(x[train, ], y[train], alpha = 0, lambda = lambda.grid, thresh = 1e-12)

# Determine best lambda, or tuning parameter, using cross-validation
set.seed(2)
cv.out <- cv.glmnet(x[train, ], y[train], alpha = 0)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam

# Predict response of test data using ridge regression and calculate MSE
ridge.pred <- predict(ridge.mod, s = bestlam, newx = x[-train, ])
ridge.mse <- mean((ridge.pred - y[-train])^2)
ridge.mse
```


### Part D - The Lasso


```{r}
# Create lasso model on the college dataset
lasso.mod <- glmnet(x[train, ], y[train], alpha = 1, lambda = lambda.grid)
plot(lasso.mod)

# Perform cross-validation to determine best lambda or tuning parameter
set.seed(3)
cv.out <- cv.glmnet(x[train, ], y[train], alpha = 1)
plot(cv.out)
bestlam2<- cv.out$lambda.min

# Predict response of test data and calculate MSE
lasso.pred <- predict(lasso.mod, s = bestlam2, newx = x[-train, ])
lasso.mse <- mean((lasso.pred - y[-train])^2)
lasso.mse
```


### Part E - PCR


```{r}
# Create PCR model on training data
set.seed(4)
pcr.fit <- pcr(Apps ~ ., data = college.data, subset = train, scale = T,
               validation = "CV")
validationplot(pcr.fit, val.type = "MSEP")
axis(side=1, at=seq(1, 20, by=1))
```


Using the cross-validation method, the lowest error occurs when M=17.


```{r}
# Predict the number of applications using the PCR model
pcr.pred <- predict(pcr.fit, x[-train, ], ncomp = 17)
pcr.mse <- mean((pcr.pred - y[-train])^2)
pcr.mse
```


### Part F - Partial Least Squares


```{r}
# Create PLS model on the college data
set.seed(5)
pls.fit <- plsr(Apps ~ ., data = college.data, subset = train, scale = T,
                validation = "CV")
summary(pls.fit)
validationplot(pls.fit, val.type = "MSEP")
axis(side=1, at=seq(1, 20, by=1))
```


```{r}
# Predict number of applications using PLS
pls.pred <- predict(pls.fit, x[-train, ], ncomp = 7)
pls.mse <- mean((pls.pred - y[-train])^2)
pls.mse
```


### Part G


```{r}
mse.models <- data.frame(
  model = c("least.squares", "ridge.regression", "lasso", "pcr", "pls"),
  mse = c(lm.mse, ridge.mse, lasso.mse, pcr.mse, pls.mse),
  stringsAsFactors = F
)
mse.models
```


Our models adequately explain the relationship between the response and the predictors. the initial least squares regression had an adjusted R-squared of 0.94 and a F-statistic over 500, which indicates that the least squares regression can be used to predict the number of applications received. Several different approaches yield a smaller test error than least squares, meaning that these approaches are even better at predicting the number of applications. The test error for PCR is significantly lower than those of the other four approaches. The other four approaches yield similar test errors to each other.


## Question 11


```{r}
boston.data <- Boston
boston.data <- na.omit(boston.data)

# Create vector half the size of college.data that contains random set of indices
set.seed(7)
train <- sample(c(TRUE, FALSE), nrow(boston.data), replace = T)
test <- (!train)
```


### Part A - Explore Different Models with Boston data


```{r}
# Best Subset Selection
regfit.full <- regsubsets(crim ~ ., data = boston.data, nvmax = 13)
reg.summary <- summary(regfit.full)
```


```{r}
par(mfrow = c(2,2))
plot(reg.summary$rss, xlab = "Number of Variables", ylab = "RSS", type = "l")
plot(reg.summary$adjr2, xlab = "Number of Variables", ylab = "Adjusted RSq",
     type = "l")
which.max(reg.summary$adjr2)
points(9, reg.summary$adjr2[9], col = "red", cex = 2, pch = 20)
```


```{r}
coef(regfit.full, 9)
```


```{r}
# Ridge Regression

x2 <- model.matrix(crim ~ ., boston.data)[, -1]
y2 <- boston.data$crim

set.seed(8)
cv.out <- cv.glmnet(x2, y2, alpha = 0)
bestlam3 <- cv.out$lambda.min

out <- glmnet(x2, y2, alpha = 0)
predict(out, type = "coefficients", s = bestlam3)[1:13,]
```


```{r}
# The Lasso
set.seed(9)
cv.out <- cv.glmnet(x2, y2, alpha = 1)
bestlam4 <- cv.out$lambda.min

out <- glmnet(x2, y2, alpha = 1, lambda = lambda.grid)
lasso.coef <- predict(out, type = "coefficients", s = bestlam4)[1:13,]
lasso.coef
```


```{r}
# PCR
set.seed(11)
pcr.boston <- pcr(crim ~ ., data = boston.data, scale = T,
                  validation = "CV")
validationplot(pcr.boston, val.type = "MSEP")
```


```{r}
# Fit PCR to entire data set using M = 8
pcr.boston <- pcr(y2 ~ x2, scale = T, ncomp = 5)
summary(pcr.boston)
```


### Part B - Propose Model for Predicting Crime Rate


```{r}
# Use Validation-Set Approach to Determine Best Subset Selection Model
regfit.best <- regsubsets(crim ~ ., data = boston.data[train, ], nvmax = 13)

# Create test matrix
test.mat <- model.matrix(crim ~ ., data = boston.data[test, ])

# Compute test MSE for all possible amounts of variables used in the model
val.errors <- rep(NA, 13)
for (i in 1:13) {
  coefi <- coef(regfit.best, id = i)
  pred <- test.mat[, names(coefi)] %*% coefi
  val.errors[i] <- mean((boston.data$crim[test] - pred)^2)
}

# Get coefficient estimates for model with best subset of variables
best.subset <- which.min(val.errors)
coef(regfit.best, best.subset)
```


Using the validation set approach on a best subset selection method, a model containing seven predictors was determined to have the lowest MSE of all combinations.


### Part C


The chosen model does not involve all features because the best subset selection method was used and with a validation set approach, we determined that the model with the lowest test MSE used only seven of the thirteen possible predictors of crime rate per capita.
